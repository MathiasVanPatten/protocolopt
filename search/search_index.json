{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"ProtocolOpt","text":"<p>ProtocolOpt is a library designed for optimizing time-dependent protocols in physical systems, particularly focusing on non-equilibrium statistical physics and thermodynamic computing applications.</p>"},{"location":"#overview","title":"Overview","text":"<p>The core of the library is the <code>ProtocolOptimizer</code>, which orchestrates the training of protocols to minimize a defined loss function (such as work or entropy production) over a simulation.</p>"},{"location":"#key-components","title":"Key Components","text":"<ul> <li>Potentials: Define the energy landscape of the system.</li> <li>Simulators: Propagate the system dynamics (e.g., Euler-Maruyama for Langevin dynamics).</li> <li>Protocols: Define how control parameters change over time.</li> <li>Losses: Define the objective function for optimization.</li> <li>Sampling: Generate initial conditions for simulations.</li> </ul> <p>Optionally, you may add callbacks to the <code>ProtocolOptimizer</code> to perform additional actions during the training process.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>See the API Reference for detailed documentation of the classes and functions.</p>"},{"location":"roadmap/","title":"Roadmap","text":"<p>Not binding, the further out the less binding as I find issues through the process.</p> <ul> <li>[ ] Replication Revisit<ul> <li>[ ] Ensure the work is as expected compared to original code for bitflip and erasure<ul> <li>[ ] Bitflip</li> <li>[ ] Bit Erasure</li> </ul> </li> <li>[ ] ensure the shape of the potential in time is as expected compared to original code for bitflip and erasure<ul> <li>[ ] Bitflip</li> <li>[ ] Bit Erasure</li> </ul> </li> </ul> </li> <li>[ ] implement grad norm with the changes to the loss api as needed https://arxiv.org/abs/1711.02257</li> <li>[ ] add schedulers </li> <li>[ ] Second QoL and Cleanup<ul> <li>[ ] Generalize training script, moving the more basic training tasks into a yaml-led process</li> <li>[~] Do an api pass, making sure everything makes sense and is as easy as possible with error checking to make sure the user is doing the right thing within reason</li> <li>[X] Optimization pass</li> <li>[X] Do a documentation pass, docstrings, typehints, comments, etc.</li> <li>[X] Add ability to save and load model</li> <li>[ ] more elegent error handling for user code to better communicate where and what went wrong without crashing the entire training run if possible</li> </ul> </li> <li>[~] add better high dimensional visualization tools</li> </ul>"},{"location":"roadmap/#some-stretch-goals","title":"Some stretch goals","text":"<ul> <li>[ ] MVP NAND, showing 2 bits<ul> <li>[ ] Loose (01 or 10 are 1)</li> <li>[X] Strict (11 is only 1)</li> </ul> </li> <li>[ ] MVP 2 bit adder, showing 4 bits (01 + 01 = 10 00 as example)</li> <li>[ ] MVP 4 bit adder, showing 8 bits (0101 + 0101 = 1010 0000 as example)</li> <li>[ ] MVP 8 bit adder, showing 16 bits (01010101 + 01010101 = 10101010 00000000 as example)</li> <li>[ ] Show different potential model<ul> <li>[ ] Spline</li> <li>[ ] Multi-layer Perceptron</li> </ul> </li> </ul>"},{"location":"reference/callbacks/","title":"Callbacks","text":""},{"location":"reference/callbacks/#protocolopt.callbacks.AimCallback","title":"<code>AimCallback</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback for experiment tracking with Aim</p> Source code in <code>src/protocolopt/callbacks/aim.py</code> <pre><code>class AimCallback(Callback):\n    \"\"\"Callback for experiment tracking with Aim\"\"\"\n\n    def __init__(self, experiment_name=None, repo_path=None, log_system_params=True, \n                 capture_terminal_logs=False, run_hash=None):\n        \"\"\"\n        Args:\n            experiment_name: Name for the experiment\n            repo_path: Path to Aim repository (defaults to current directory)\n            log_system_params: Whether to log system parameters (Installed packages, git info, env vars, NOT GPU/CPU info which is always logged)\n            capture_terminal_logs: Whether to capture terminal output\n            run_hash: Optional hash to continue a previous run\n        \"\"\"\n        if not AIM_AVAILABLE:\n            raise ImportError(\"Aim is not installed. Install with 'pip install aim'\")\n\n        self.experiment_name = experiment_name\n        self.repo_path = repo_path\n        self.log_system_params = log_system_params\n        self.capture_terminal_logs = capture_terminal_logs\n        self.run_hash = run_hash\n        self.run = None\n\n    def _sanitize_value(self, value: Any) -&gt; Any:\n        if isinstance(value, torch.Tensor):\n            return value.item() if value.numel() == 1 else value.tolist()\n        elif isinstance(value, dict):\n            return {k: self._sanitize_value(v) for k, v in value.items()}\n        elif isinstance(value, (list, tuple)):\n            return [self._sanitize_value(v) for v in value]\n        return value\n\n    def on_train_start(self, optimizer: \"ProtocolOptimizer\") -&gt; None:\n        self.run = Run(\n            repo=self.repo_path,\n            experiment=self.experiment_name,\n            run_hash=self.run_hash,\n            log_system_params=self.log_system_params,\n            capture_terminal_logs=self.capture_terminal_logs,\n        )\n\n        config = {}\n\n        components = {\n            'optimizer': optimizer,\n            'potential': optimizer.potential,\n            'simulator': optimizer.simulator,\n            'loss': optimizer.loss,\n            'protocol': optimizer.protocol,\n            'initial_condition_generator': optimizer.init_cond_generator,\n        }\n\n        for name, comp in components.items():\n            if hasattr(comp, 'hparams'):\n                config[name] = self._sanitize_value(comp.hparams)\n            elif hasattr(comp, '__dict__'):\n                config[name] = {k: self._sanitize_value(v) for k, v in comp.__dict__.items() \n                              if isinstance(v, (int, float, str, bool, torch.Tensor))}\n\n        def get_class_name(obj):\n            if isinstance(obj, type):\n                return obj.__name__\n            return obj.__class__.__name__\n\n        config['optimizer'] = config['optimizer'] | {\n            'optimizer': get_class_name(optimizer.optimizer_class),\n            'optimizer_kwargs': self._sanitize_value(optimizer.optimizer_kwargs),\n            'scheduler': get_class_name(optimizer.scheduler_class) if optimizer.scheduler_class else None,\n            'scheduler_kwargs': self._sanitize_value(optimizer.scheduler_kwargs),\n            'scheduler_restart_decay': optimizer.scheduler_restart_decay\n        }\n        self.run['hparams'] = config\n\n    def on_epoch_end(self, optimizer: \"ProtocolOptimizer\", sim_dict: Dict[str, Any], \n                     loss_values: torch.Tensor, epoch: int) -&gt; None:\n\n        self.run.track(loss_values.mean().item(), name='loss', epoch=epoch, context={'agg': 'mean'})\n        self.run.track(loss_values.std().item(), name='loss', epoch=epoch, context={'agg': 'std'})\n        self.run.track(loss_values.min().item(), name='loss', epoch=epoch, context={'agg': 'min'})\n        self.run.track(loss_values.max().item(), name='loss', epoch=epoch, context={'agg': 'max'})\n\n        if hasattr(optimizer.loss, 'log_components'):\n            metrics = optimizer.loss.log_components(\n                sim_dict['potential'], \n                sim_dict['microstate_paths'],\n                sim_dict['dw_tensor'],\n                sim_dict['protocol_tensor'], \n                optimizer.simulator.dt\n            )\n\n            for metric_name, value in metrics.items():\n                val = value.mean().item() if isinstance(value, torch.Tensor) else value\n\n                ctx = {'group': 'loss_components'} if 'loss' in metric_name else {'group': 'metrics'}\n\n                self.run.track(val, name=metric_name, epoch=epoch, context=ctx)\n\n    def track_figure(self, figure: Any, name: str, epoch: int, context: Dict[str, Any] = {}, dpi: int = 150) -&gt; None:\n        \"\"\"\n        Track a matplotlib figure as an Aim Image.\n\n        Args:\n            figure: Matplotlib figure to track\n            name: Name of the image\n            epoch: Current epoch\n            context: Optional context dictionary\n            dpi: DPI for the saved image\n        \"\"\"\n        if not self.run:\n            return\n\n        aim_image = AimImage(figure)\n        self.run.track(aim_image, name=name, epoch=epoch, context=context)\n\n    def on_train_end(self, *args, **kwargs) -&gt; None:\n        if self.run:\n            self.run.close()\n</code></pre>"},{"location":"reference/callbacks/#protocolopt.callbacks.AimCallback.__init__","title":"<code>__init__(experiment_name=None, repo_path=None, log_system_params=True, capture_terminal_logs=False, run_hash=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>experiment_name</code> <p>Name for the experiment</p> <code>None</code> <code>repo_path</code> <p>Path to Aim repository (defaults to current directory)</p> <code>None</code> <code>log_system_params</code> <p>Whether to log system parameters (Installed packages, git info, env vars, NOT GPU/CPU info which is always logged)</p> <code>True</code> <code>capture_terminal_logs</code> <p>Whether to capture terminal output</p> <code>False</code> <code>run_hash</code> <p>Optional hash to continue a previous run</p> <code>None</code> Source code in <code>src/protocolopt/callbacks/aim.py</code> <pre><code>def __init__(self, experiment_name=None, repo_path=None, log_system_params=True, \n             capture_terminal_logs=False, run_hash=None):\n    \"\"\"\n    Args:\n        experiment_name: Name for the experiment\n        repo_path: Path to Aim repository (defaults to current directory)\n        log_system_params: Whether to log system parameters (Installed packages, git info, env vars, NOT GPU/CPU info which is always logged)\n        capture_terminal_logs: Whether to capture terminal output\n        run_hash: Optional hash to continue a previous run\n    \"\"\"\n    if not AIM_AVAILABLE:\n        raise ImportError(\"Aim is not installed. Install with 'pip install aim'\")\n\n    self.experiment_name = experiment_name\n    self.repo_path = repo_path\n    self.log_system_params = log_system_params\n    self.capture_terminal_logs = capture_terminal_logs\n    self.run_hash = run_hash\n    self.run = None\n</code></pre>"},{"location":"reference/callbacks/#protocolopt.callbacks.AimCallback.track_figure","title":"<code>track_figure(figure, name, epoch, context={}, dpi=150)</code>","text":"<p>Track a matplotlib figure as an Aim Image.</p> <p>Parameters:</p> Name Type Description Default <code>figure</code> <code>Any</code> <p>Matplotlib figure to track</p> required <code>name</code> <code>str</code> <p>Name of the image</p> required <code>epoch</code> <code>int</code> <p>Current epoch</p> required <code>context</code> <code>Dict[str, Any]</code> <p>Optional context dictionary</p> <code>{}</code> <code>dpi</code> <code>int</code> <p>DPI for the saved image</p> <code>150</code> Source code in <code>src/protocolopt/callbacks/aim.py</code> <pre><code>def track_figure(self, figure: Any, name: str, epoch: int, context: Dict[str, Any] = {}, dpi: int = 150) -&gt; None:\n    \"\"\"\n    Track a matplotlib figure as an Aim Image.\n\n    Args:\n        figure: Matplotlib figure to track\n        name: Name of the image\n        epoch: Current epoch\n        context: Optional context dictionary\n        dpi: DPI for the saved image\n    \"\"\"\n    if not self.run:\n        return\n\n    aim_image = AimImage(figure)\n    self.run.track(aim_image, name=name, epoch=epoch, context=context)\n</code></pre>"},{"location":"reference/callbacks/#protocolopt.callbacks.ConfusionMatrixCallback","title":"<code>ConfusionMatrixCallback</code>","text":"<p>               Bases: <code>BasePlottingCallback</code></p> <p>Callback for plotting confusion matrix of binary state transitions</p> Source code in <code>src/protocolopt/callbacks/plotting.py</code> <pre><code>class ConfusionMatrixCallback(BasePlottingCallback):\n    \"\"\"Callback for plotting confusion matrix of binary state transitions\"\"\"\n\n    def __init__(self, save_dir='figs', plot_frequency=None):\n        \"\"\"\n        Args:\n            save_dir: Directory to save plots\n            plot_frequency: How often to plot (e.g., every N epochs). If None, plots at 0, 25%, 50%, 75%, 100%\n        \"\"\"\n        self.save_dir = Path(save_dir)\n        self.save_dir.mkdir(parents=True, exist_ok=True)\n        self.plot_frequency = plot_frequency\n        self.total_epochs = None\n\n    def _plot(self, optimizer_object: \"ProtocolOptimizer\", sim_dict: Dict[str, Any], epoch: int) -&gt; None:\n        # Get the loss object\n        loss_object = optimizer_object.loss\n\n        # Check if loss supports binary trajectory computation\n        if not hasattr(loss_object, 'compute_binary_trajectory_info'):\n            return  # Only works with LogicGateEndpointLossBase-derived losses\n\n        # Compute binary trajectory information\n        binary_trajectory_dict = loss_object.compute_binary_trajectory_info(sim_dict['microstate_paths'])\n        starting_bits_int = binary_trajectory_dict['starting_bits_int'].cpu().numpy()\n        ending_bits_int = binary_trajectory_dict['ending_bits_int'].cpu().numpy()\n\n        # Get the domain size (number of possible states)\n        if not hasattr(loss_object, 'domain'):\n            return\n\n        domain = loss_object.domain\n        validity = loss_object.validity.detach().cpu().numpy()\n\n        # Create confusion matrix (counts)\n        confusion_matrix = torch.zeros(domain, domain, dtype=torch.float32)\n        for start, end in zip(starting_bits_int, ending_bits_int):\n            confusion_matrix[start, end] += 1\n\n        # Create figure\n        fig, ax = plt.subplots(figsize=(10, 8))\n\n        valid_color = np.array([144, 238, 144]) / 255  # Light green\n        invalid_color = np.array([255, 182, 193]) / 255  # Light pink\n\n        background = np.zeros((domain, domain, 3))\n\n        for i in range(domain):\n            for j in range(domain):\n                is_valid = validity[i, j]\n\n                if is_valid:\n                    background[i, j] = valid_color\n                else:\n                    background[i, j] = invalid_color\n\n        # Display the background\n        ax.imshow(background, aspect='auto')\n\n        # Add text annotations with counts\n        for i in range(domain):\n            for j in range(domain):\n                count = int(confusion_matrix[i, j].item())\n                is_valid = validity[i, j]\n\n                # Use dark text for visibility on light backgrounds\n                text_color = 'black'\n\n                text = f'{count}'\n\n                ax.text(j, i, text, ha='center', va='center', \n                       color=text_color, fontsize=11, weight='bold')\n\n        # Format binary labels\n        bit_width = len(bin(domain - 1)) - 2  # Number of bits needed\n        labels = [format(i, f'0{bit_width}b') for i in range(domain)]\n\n        ax.set_xticks(range(domain))\n        ax.set_yticks(range(domain))\n        ax.set_xticklabels(labels)\n        ax.set_yticklabels(labels)\n        ax.set_xlabel('Output Bit State')\n        ax.set_ylabel('Input Bit State')\n        ax.set_title(f'Binary State Transition Confusion Matrix (Epoch {epoch})\\nGreen=Valid, Red=Invalid')\n\n        plt.tight_layout()\n        self._log_or_save_figure(fig, 'confusion_matrix', epoch, optimizer_object, dpi=150)\n        plt.close()\n</code></pre>"},{"location":"reference/callbacks/#protocolopt.callbacks.ConfusionMatrixCallback.__init__","title":"<code>__init__(save_dir='figs', plot_frequency=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>save_dir</code> <p>Directory to save plots</p> <code>'figs'</code> <code>plot_frequency</code> <p>How often to plot (e.g., every N epochs). If None, plots at 0, 25%, 50%, 75%, 100%</p> <code>None</code> Source code in <code>src/protocolopt/callbacks/plotting.py</code> <pre><code>def __init__(self, save_dir='figs', plot_frequency=None):\n    \"\"\"\n    Args:\n        save_dir: Directory to save plots\n        plot_frequency: How often to plot (e.g., every N epochs). If None, plots at 0, 25%, 50%, 75%, 100%\n    \"\"\"\n    self.save_dir = Path(save_dir)\n    self.save_dir.mkdir(parents=True, exist_ok=True)\n    self.plot_frequency = plot_frequency\n    self.total_epochs = None\n</code></pre>"},{"location":"reference/callbacks/#protocolopt.callbacks.PotentialLandscapePlotCallback","title":"<code>PotentialLandscapePlotCallback</code>","text":"<p>               Bases: <code>BasePlottingCallback</code></p> Source code in <code>src/protocolopt/callbacks/plotting.py</code> <pre><code>class PotentialLandscapePlotCallback(BasePlottingCallback):\n\n    def __init__(self, save_dir='figs', plot_frequency=None, spatial_resolution=100, trajectories_per_bit=10):\n        \"\"\"\n        Args:\n            save_dir: Directory to save plots if Aim is not available.\n            plot_frequency: How often to plot (e.g., every N epochs).\n            spatial_resolution: Number of points to grid the spatial dimension for potential evaluation.\n            trajectories_per_bit: Number of trajectories to overlay per bit state.\n        \"\"\"\n        self.save_dir = Path(save_dir)\n        self.plot_frequency = plot_frequency\n        self.spatial_resolution = spatial_resolution\n        self.trajectories_per_bit = trajectories_per_bit\n        self.total_epochs = None\n\n    def _plot(self, optimizer_object: \"ProtocolOptimizer\", sim_dict: Dict[str, Any], epoch: int) -&gt; None:\n        protocol_tensor = sim_dict.get('protocol_tensor', None)\n        if protocol_tensor is None:\n            return\n\n        microstate_paths: MicrostatePaths = sim_dict['microstate_paths']\n        spatial_dimensions = optimizer_object.init_cond_generator.spatial_dimensions\n        time_steps = optimizer_object.protocol.time_steps\n        spatial_bounds = getattr(optimizer_object.init_cond_generator, 'mcmc_starting_spatial_bounds', None)\n        if spatial_bounds is None:\n            min_vals = microstate_paths[..., 0].min(dim=0)[0].min(dim=1)[0]\n            max_vals = microstate_paths[..., 0].max(dim=0)[0].max(dim=1)[0]\n\n            centers = (max_vals + min_vals) / 2\n            spans = (max_vals - min_vals)\n\n\n            spans = torch.maximum(spans, torch.tensor(1.0, device=optimizer_object.device))\n\n            x_min_calc = centers - spans\n            x_max_calc = centers + spans\n\n            spatial_bounds = torch.stack([x_min_calc, x_max_calc], dim=1)\n\n        bit_locations = optimizer_object.loss.bit_locations\n        potential_obj = optimizer_object.potential\n\n        for dim_idx in range(spatial_dimensions):\n            x_min, x_max = spatial_bounds[dim_idx, 0].item(), spatial_bounds[dim_idx, 1].item()\n            x_grid = torch.linspace(x_min, x_max, self.spatial_resolution, device=optimizer_object.device)\n            t_indices = torch.arange(time_steps, device=optimizer_object.device)\n\n            query_points = torch.zeros(self.spatial_resolution, spatial_dimensions, device=optimizer_object.device)\n            query_points[:, dim_idx] = x_grid\n\n            protocol_time_points = protocol_tensor.shape[1]\n            potential_values = torch.zeros(self.spatial_resolution, protocol_time_points)\n            for t_idx in range(protocol_time_points):\n                coeff_slice = protocol_tensor[:, t_idx]\n                potential_values[:, t_idx] = potential_obj.potential_value(query_points, coeff_slice).cpu()\n\n            potential_values = potential_values.sign() * ((potential_values.abs() + 1).log10())\n            path_positions = microstate_paths[:, dim_idx, :, 0]\n\n            traj_min = path_positions.min().item()\n            traj_max = path_positions.max().item()\n            spatial_buffer = (traj_max - traj_min) * 0.2\n\n            mask = (x_grid.cpu() &gt;= (traj_min - spatial_buffer)) &amp; (x_grid.cpu() &lt;= (traj_max + spatial_buffer))\n            relevant_potential = potential_values[mask, :]\n\n            v_min = torch.quantile(relevant_potential, 0.05).item()\n            v_max = torch.quantile(relevant_potential, 0.95).item()\n            start_positions = path_positions[:, 0]\n            bit_locs_dim = bit_locations[:, dim_idx]\n            distances = torch.abs(start_positions[:, None] - bit_locs_dim[None, :])\n            bit_assignments = distances.argmin(dim=1)\n\n            sampled_indices = []\n            num_bits = bit_locs_dim.shape[0]\n            for bit_idx in range(num_bits):\n                mask = (bit_assignments == bit_idx)\n                indices_in_class = torch.where(mask)[0]\n                if len(indices_in_class) &gt; 0:\n                    num_to_sample = min(self.trajectories_per_bit, len(indices_in_class))\n                    sampled = indices_in_class[torch.randperm(len(indices_in_class))[:num_to_sample]]\n                    sampled_indices.append(sampled)\n            if len(sampled_indices) &gt; 0:\n                sampled_indices = torch.cat(sampled_indices)\n            else:\n                sampled_indices = torch.tensor([], dtype=torch.long)\n\n            fig, ax = plt.subplots(figsize=(10, 6))\n            im = ax.imshow(potential_values.T, aspect='auto', origin='lower', \n                          extent=[x_min, x_max, 0, protocol_time_points], cmap='viridis', \n                          vmin=v_min, vmax=v_max)\n            ax.set_xlabel('Position')\n            ax.set_ylabel('Time')\n            ax.set_title(f'Potential Landscape - Dim {dim_idx} (Epoch {epoch})')\n            plt.colorbar(im, ax=ax, label='Signed log10(Potential Value)')\n\n\n            high_contrast_colors = ['cyan', 'magenta', 'lime', 'orange', 'white']\n\n            for idx in sampled_indices:\n                bit_class = bit_assignments[idx].item()\n                traj = path_positions[idx].cpu().numpy()\n                time_array = np.linspace(0, time_steps, time_steps + 1)\n\n                color_idx = bit_class % len(high_contrast_colors)\n                color = high_contrast_colors[color_idx]\n\n                ax.plot(traj, time_array, color=color, alpha=0.8, linewidth=1.5)\n\n            self._log_or_save_figure(fig, f'potential_landscape', epoch, optimizer_object, context={'dim': dim_idx})\n            plt.close(fig)\n</code></pre>"},{"location":"reference/callbacks/#protocolopt.callbacks.PotentialLandscapePlotCallback.__init__","title":"<code>__init__(save_dir='figs', plot_frequency=None, spatial_resolution=100, trajectories_per_bit=10)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>save_dir</code> <p>Directory to save plots if Aim is not available.</p> <code>'figs'</code> <code>plot_frequency</code> <p>How often to plot (e.g., every N epochs).</p> <code>None</code> <code>spatial_resolution</code> <p>Number of points to grid the spatial dimension for potential evaluation.</p> <code>100</code> <code>trajectories_per_bit</code> <p>Number of trajectories to overlay per bit state.</p> <code>10</code> Source code in <code>src/protocolopt/callbacks/plotting.py</code> <pre><code>def __init__(self, save_dir='figs', plot_frequency=None, spatial_resolution=100, trajectories_per_bit=10):\n    \"\"\"\n    Args:\n        save_dir: Directory to save plots if Aim is not available.\n        plot_frequency: How often to plot (e.g., every N epochs).\n        spatial_resolution: Number of points to grid the spatial dimension for potential evaluation.\n        trajectories_per_bit: Number of trajectories to overlay per bit state.\n    \"\"\"\n    self.save_dir = Path(save_dir)\n    self.plot_frequency = plot_frequency\n    self.spatial_resolution = spatial_resolution\n    self.trajectories_per_bit = trajectories_per_bit\n    self.total_epochs = None\n</code></pre>"},{"location":"reference/callbacks/#protocolopt.callbacks.ProtocolPlotCallback","title":"<code>ProtocolPlotCallback</code>","text":"<p>               Bases: <code>BasePlottingCallback</code></p> Source code in <code>src/protocolopt/callbacks/plotting.py</code> <pre><code>class ProtocolPlotCallback(BasePlottingCallback):\n\n    def __init__(self, save_dir='figs', plot_frequency=None):\n        \"\"\"\n        Args:\n            save_dir: Directory to save plots if Aim is not available.\n            plot_frequency: How often to plot (e.g., every N epochs).\n        \"\"\"\n        self.save_dir = Path(save_dir)\n        self.plot_frequency = plot_frequency\n        self.total_epochs = None\n\n    def _plot(self, optimizer_object: \"ProtocolOptimizer\", sim_dict: Dict[str, Any], epoch: int) -&gt; None:\n        protocol_tensor = sim_dict.get('protocol_tensor', None)\n        if protocol_tensor is None:\n            return\n\n        protocol_tensor_cpu = protocol_tensor.cpu().numpy()\n        control_dim, time_steps = protocol_tensor_cpu.shape\n        time_array = np.arange(time_steps)\n\n        if control_dim &lt;= 4:\n            ncols = control_dim\n            nrows = 1\n        else:\n            ncols = 2\n            nrows = (control_dim + 1) // 2\n\n        fig, axes = plt.subplots(nrows, ncols, figsize=(5*ncols, 4*nrows), squeeze=False)\n        axes = axes.flatten()\n\n        for coeff_idx in range(control_dim):\n            ax = axes[coeff_idx]\n            ax.plot(time_array, protocol_tensor_cpu[coeff_idx, :])\n            ax.set_xlabel('Time')\n            ax.set_ylabel(f'Coeff {coeff_idx}')\n            ax.set_title(f'Control Var {coeff_idx}')\n            ax.grid(True, alpha=0.3)\n\n        for idx in range(control_dim, len(axes)):\n            axes[idx].set_visible(False)\n\n        plt.tight_layout()\n        self._log_or_save_figure(fig, 'protocol_control_time_evolution', epoch, optimizer_object)\n        plt.close(fig)\n</code></pre>"},{"location":"reference/callbacks/#protocolopt.callbacks.ProtocolPlotCallback.__init__","title":"<code>__init__(save_dir='figs', plot_frequency=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>save_dir</code> <p>Directory to save plots if Aim is not available.</p> <code>'figs'</code> <code>plot_frequency</code> <p>How often to plot (e.g., every N epochs).</p> <code>None</code> Source code in <code>src/protocolopt/callbacks/plotting.py</code> <pre><code>def __init__(self, save_dir='figs', plot_frequency=None):\n    \"\"\"\n    Args:\n        save_dir: Directory to save plots if Aim is not available.\n        plot_frequency: How often to plot (e.g., every N epochs).\n    \"\"\"\n    self.save_dir = Path(save_dir)\n    self.plot_frequency = plot_frequency\n    self.total_epochs = None\n</code></pre>"},{"location":"reference/callbacks/#protocolopt.callbacks.TrajectoryPlotCallback","title":"<code>TrajectoryPlotCallback</code>","text":"<p>               Bases: <code>BasePlottingCallback</code></p> <p>Callback for plotting microstate paths over time</p> Source code in <code>src/protocolopt/callbacks/plotting.py</code> <pre><code>class TrajectoryPlotCallback(BasePlottingCallback):\n    \"\"\"Callback for plotting microstate paths over time\"\"\"\n\n    def __init__(self, save_dir='figs', plot_frequency=None, num_trajectories=100):\n        \"\"\"\n        Args:\n            save_dir: Directory to save plots (relative to working directory or absolute path)\n            plot_frequency: How often to plot (e.g., every N epochs). If None, plots at 0, 25%, 50%, 75%, 100%\n            num_trajectories: Number of random paths to plot (default: 100)\n        \"\"\"\n        self.save_dir = Path(save_dir)\n        self.save_dir.mkdir(parents=True, exist_ok=True)\n        self.plot_frequency = plot_frequency\n        self.num_trajectories = num_trajectories\n        self.total_epochs = None\n\n    def _plot(self, optimizer_object: \"ProtocolOptimizer\", sim_dict: Dict[str, Any], epoch: int) -&gt; None:\n        time_steps = optimizer_object.protocol.time_steps\n        microstate_paths: MicrostatePaths = sim_dict['microstate_paths']\n        spatial_dimensions = optimizer_object.init_cond_generator.spatial_dimensions\n\n        # Choose random path indices\n        num_paths = microstate_paths.shape[0]\n        sample_size = min(self.num_trajectories, num_paths)\n        random_indices = torch.randperm(num_paths)[:sample_size]\n\n        # Create separate plot for each spatial dimension\n        for dim_idx in range(spatial_dimensions):\n            # Extract position data for this dimension\n            path_positions = microstate_paths[random_indices, dim_idx, :, 0].cpu()\n\n            fig = plt.figure()\n            plt.plot(torch.linspace(0, time_steps, time_steps + 1).cpu(), path_positions.T)\n            plt.xlabel('Time')\n            plt.ylabel(f'Position (Dimension {dim_idx})')\n            plt.title(f'Microstate Positions Over Time - Dim {dim_idx} (Epoch {epoch})')\n            self._log_or_save_figure(fig, f'trajectory_dim', epoch, optimizer_object, context={'dim': dim_idx})\n            plt.close()\n</code></pre>"},{"location":"reference/callbacks/#protocolopt.callbacks.TrajectoryPlotCallback.__init__","title":"<code>__init__(save_dir='figs', plot_frequency=None, num_trajectories=100)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>save_dir</code> <p>Directory to save plots (relative to working directory or absolute path)</p> <code>'figs'</code> <code>plot_frequency</code> <p>How often to plot (e.g., every N epochs). If None, plots at 0, 25%, 50%, 75%, 100%</p> <code>None</code> <code>num_trajectories</code> <p>Number of random paths to plot (default: 100)</p> <code>100</code> Source code in <code>src/protocolopt/callbacks/plotting.py</code> <pre><code>def __init__(self, save_dir='figs', plot_frequency=None, num_trajectories=100):\n    \"\"\"\n    Args:\n        save_dir: Directory to save plots (relative to working directory or absolute path)\n        plot_frequency: How often to plot (e.g., every N epochs). If None, plots at 0, 25%, 50%, 75%, 100%\n        num_trajectories: Number of random paths to plot (default: 100)\n    \"\"\"\n    self.save_dir = Path(save_dir)\n    self.save_dir.mkdir(parents=True, exist_ok=True)\n    self.plot_frequency = plot_frequency\n    self.num_trajectories = num_trajectories\n    self.total_epochs = None\n</code></pre>"},{"location":"reference/core/","title":"Core","text":""},{"location":"reference/losses/","title":"Losses","text":""},{"location":"reference/losses/#protocolopt.losses.LogicGateEndpointLossBase","title":"<code>LogicGateEndpointLossBase</code>","text":"<p>               Bases: <code>Loss</code></p> <p>Base class for losses that depend on the final state of the trajectory relative to target bits.</p> Source code in <code>src/protocolopt/losses/standard.py</code> <pre><code>class LogicGateEndpointLossBase(Loss):\n    \"\"\"Base class for losses that depend on the final state of the trajectory relative to target bits.\"\"\"\n\n    def __init__(self, midpoints: torch.Tensor, truth_table: Dict[int, Union[List[str], Dict]], bit_locations: torch.Tensor, exponent: int = 2, starting_bit_weights: Optional[torch.Tensor] = None):\n        \"\"\"Initializes the LogicGateEndpointLossBase.\n\n        Args:\n            midpoints: Midpoints defining bit boundaries in each spatial dimension. Currently only\n                supports binary mapping. Should be a vector of midpoints for each spatial dimension.\n            truth_table: Dictionary defining valid transitions. It should be nested with one input bit\n                at a time as the keys with the innermost level showing the acceptable outputs (as a list\n                of bit strings) for that bit configuration. You must fully define the input/output space\n                in a 1-Many or 1-1 enforced configuration.\n            bit_locations: Target locations for each bit configuration. It is assumed that index 0 is\n                the location of 0b0, index 1 is the location of 0b1, etc. To the left of the midpoint\n                is 0 and the right is 1. Exactly on the midpoint maps to 0.\n                Shape: (Domain_Size, Spatial_Dim)\n                Domain_Size is 2^Spatial_Dim.\n            exponent: Exponent for the distance metric (p-norm).\n            starting_bit_weights: Optional tensor of weights for each starting state (int representation).\n                Shape should match domain size.\n\n        Examples:\n            NAND with 2 bits with the rightmost bit being used as the output:\n\n            {\n                0: {\n                    0: ['01', '11'],\n                    1: ['01', '11']\n                },\n                1: {\n                    0: ['01', '11'],\n                    1: ['00', '10']\n                }\n            }\n\n            NAND where only 11 is used as 1:\n\n            {\n                0: {\n                    0: ['11'],\n                    1: ['11']\n                },\n                1: {\n                    0: ['11'],\n                    1: ['00', '10', '01']\n                }\n            }\n        \"\"\"\n\n        self.midpoints = midpoints\n        self.bit_locations = bit_locations\n        self.exponent = exponent\n        self.starting_bit_weights = starting_bit_weights\n        self.truth_table = truth_table\n        self._validate_input_sequence_in_truth_table(truth_table)\n        self.domain = 2**self._get_depth_of_truth_table(truth_table)\n        self.flattened_truth_table = {k : None for k in range(self.domain)}\n        self._flatten_truth_table(truth_table)\n        self._gen_validity_mapping()\n        self.hparams = {\n            'midpoints': self.midpoints.tolist() if isinstance(self.midpoints, torch.Tensor) else self.midpoints,\n            'truth_table': self.truth_table,\n            'bit_locations_shape': list(self.bit_locations.shape),\n            'exponent': self.exponent,\n            'starting_bit_weights': self.starting_bit_weights.tolist() if self.starting_bit_weights is not None else None,\n            'domain': self.domain,\n            'name': self.__class__.__name__\n        }\n\n    def _get_depth_of_truth_table(self, truth_table_dict):\n        if not isinstance(truth_table_dict[0], dict):\n            return 1\n        else:\n            return 1 + self._get_depth_of_truth_table(truth_table_dict[0])\n\n    def _compute_starting_bits_int(self, microstate_paths: MicrostatePaths) -&gt; torch.Tensor:\n        starting_bits = microstate_paths[:,:,0,0] &gt; self.midpoints[None,:]\n        starting_bits_int = torch.sum(starting_bits.int() * torch.tensor(list(reversed([2**x for x in range(self.midpoints.shape[0])])), device = microstate_paths.device)[None, :], axis = -1)\n        return starting_bits_int\n\n    def _compute_ending_bits_int(self, microstate_paths: MicrostatePaths) -&gt; torch.Tensor:\n        ending_bits = microstate_paths[:,:,-1,0] &gt; self.midpoints[None,:]\n        ending_bits_int = torch.sum(ending_bits.int() * torch.tensor(list(reversed([2**x for x in range(self.midpoints.shape[0])])), device = microstate_paths.device)[None, :], axis = -1)\n        return ending_bits_int\n\n    def _endpoint_loss(self, microstate_paths: MicrostatePaths) -&gt; torch.Tensor:\n        starting_bits_int = self._compute_starting_bits_int(microstate_paths)\n        ending_positions = microstate_paths[:,:,-1,0]\n        distances = torch.norm(ending_positions[:,None,:] - self.bit_locations[None,:,:], dim = -1, p = self.exponent)\n        valid_mask = self.validity[starting_bits_int, :]\n        masked_distances = torch.where(valid_mask, distances, torch.inf)\n\n        # Get minimum distance to a valid target state\n        min_distances = masked_distances.min(axis = -1).values\n\n        # Apply starting bit weights if provided\n        if self.starting_bit_weights is not None:\n            weights = self.starting_bit_weights[starting_bits_int]\n            return min_distances * weights\n\n        return min_distances\n\n\n    def _gen_validity_mapping(self):\n        self.validity = torch.zeros(self.domain, self.domain, dtype = torch.bool, device = self.bit_locations.device)\n        for inputstate, outputstate in self.flattened_truth_table.items():\n            self.validity[inputstate, outputstate] = True\n\n    def compute_binary_trajectory_info(self, microstate_paths: MicrostatePaths) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"\n        Compute starting and ending binary states for microstate paths.\n\n        Args:\n            microstate_paths: Microstate paths tensor.\n                              Shape: (Batch, Spatial_Dim, Time_Steps+1, 2)\n\n        Returns:\n            Dictionary with 'starting_bits_int' and 'ending_bits_int' tensors\n        \"\"\"\n        return {\n            'starting_bits_int': self._compute_starting_bits_int(microstate_paths),\n            'ending_bits_int': self._compute_ending_bits_int(microstate_paths)\n        }\n\n    def compute_binary_error_rate(self, microstate_paths: MicrostatePaths) -&gt; float:\n        \"\"\"\n        Compute the percentage of microstate paths ending in invalid states.\n\n        Args:\n            microstate_paths: Microstate paths tensor (should be detached)\n                              Shape: (Batch, Spatial_Dim, Time_Steps+1, 2)\n\n        Returns:\n            Float representing the error rate (0.0 to 1.0)\n        \"\"\"\n        starting_bits_int = self._compute_starting_bits_int(microstate_paths)\n        ending_bits_int = self._compute_ending_bits_int(microstate_paths)\n        valid_transitions = self.validity[starting_bits_int, ending_bits_int]\n        error_rate = (~valid_transitions).float().mean()\n        return error_rate.item()\n\n    def _validate_input_sequence_in_truth_table(self, truth_table_dict, current_bit_sequence = ''):\n        for bit in [0, 1]:\n            if bit not in truth_table_dict:\n                raise TruthTableError(f\"Missing key {bit}\", current_bit_sequence)\n            if isinstance(truth_table_dict[bit], dict):\n                try:\n                    self._validate_input_sequence_in_truth_table(\n                        truth_table_dict[bit],\n                        current_bit_sequence = current_bit_sequence + str(bit)\n                    )\n                except TruthTableError as e:\n                    raise\n                except Exception as e:\n                    raise TruthTableError(f\"Unexpected error ({type(e).__name__}: {e})\", current_bit_sequence + str(bit)) from e\n\n    def _flatten_truth_table(self, truth_table_dict, current_bit_sequence = ''):\n        for bit in [0, 1]:\n            if isinstance(truth_table_dict[bit], dict):\n                self._flatten_truth_table(truth_table_dict[bit], current_bit_sequence + str(bit))\n            else:\n                list_to_add = [int(i, base = 2) for i in truth_table_dict[bit]]\n                if list_to_add == []:\n                    raise TruthTableError(f\"Missing output for {bit}\", current_bit_sequence + str(bit))\n                self.flattened_truth_table[int(current_bit_sequence + str(bit), base = 2)] = list_to_add\n</code></pre>"},{"location":"reference/losses/#protocolopt.losses.LogicGateEndpointLossBase.__init__","title":"<code>__init__(midpoints, truth_table, bit_locations, exponent=2, starting_bit_weights=None)</code>","text":"<p>Initializes the LogicGateEndpointLossBase.</p> <p>Parameters:</p> Name Type Description Default <code>midpoints</code> <code>Tensor</code> <p>Midpoints defining bit boundaries in each spatial dimension. Currently only supports binary mapping. Should be a vector of midpoints for each spatial dimension.</p> required <code>truth_table</code> <code>Dict[int, Union[List[str], Dict]]</code> <p>Dictionary defining valid transitions. It should be nested with one input bit at a time as the keys with the innermost level showing the acceptable outputs (as a list of bit strings) for that bit configuration. You must fully define the input/output space in a 1-Many or 1-1 enforced configuration.</p> required <code>bit_locations</code> <code>Tensor</code> <p>Target locations for each bit configuration. It is assumed that index 0 is the location of 0b0, index 1 is the location of 0b1, etc. To the left of the midpoint is 0 and the right is 1. Exactly on the midpoint maps to 0. Shape: (Domain_Size, Spatial_Dim) Domain_Size is 2^Spatial_Dim.</p> required <code>exponent</code> <code>int</code> <p>Exponent for the distance metric (p-norm).</p> <code>2</code> <code>starting_bit_weights</code> <code>Optional[Tensor]</code> <p>Optional tensor of weights for each starting state (int representation). Shape should match domain size.</p> <code>None</code> <p>Examples:</p> <p>NAND with 2 bits with the rightmost bit being used as the output:</p> <p>{     0: {         0: ['01', '11'],         1: ['01', '11']     },     1: {         0: ['01', '11'],         1: ['00', '10']     } }</p> <p>NAND where only 11 is used as 1:</p> <p>{     0: {         0: ['11'],         1: ['11']     },     1: {         0: ['11'],         1: ['00', '10', '01']     } }</p> Source code in <code>src/protocolopt/losses/standard.py</code> <pre><code>def __init__(self, midpoints: torch.Tensor, truth_table: Dict[int, Union[List[str], Dict]], bit_locations: torch.Tensor, exponent: int = 2, starting_bit_weights: Optional[torch.Tensor] = None):\n    \"\"\"Initializes the LogicGateEndpointLossBase.\n\n    Args:\n        midpoints: Midpoints defining bit boundaries in each spatial dimension. Currently only\n            supports binary mapping. Should be a vector of midpoints for each spatial dimension.\n        truth_table: Dictionary defining valid transitions. It should be nested with one input bit\n            at a time as the keys with the innermost level showing the acceptable outputs (as a list\n            of bit strings) for that bit configuration. You must fully define the input/output space\n            in a 1-Many or 1-1 enforced configuration.\n        bit_locations: Target locations for each bit configuration. It is assumed that index 0 is\n            the location of 0b0, index 1 is the location of 0b1, etc. To the left of the midpoint\n            is 0 and the right is 1. Exactly on the midpoint maps to 0.\n            Shape: (Domain_Size, Spatial_Dim)\n            Domain_Size is 2^Spatial_Dim.\n        exponent: Exponent for the distance metric (p-norm).\n        starting_bit_weights: Optional tensor of weights for each starting state (int representation).\n            Shape should match domain size.\n\n    Examples:\n        NAND with 2 bits with the rightmost bit being used as the output:\n\n        {\n            0: {\n                0: ['01', '11'],\n                1: ['01', '11']\n            },\n            1: {\n                0: ['01', '11'],\n                1: ['00', '10']\n            }\n        }\n\n        NAND where only 11 is used as 1:\n\n        {\n            0: {\n                0: ['11'],\n                1: ['11']\n            },\n            1: {\n                0: ['11'],\n                1: ['00', '10', '01']\n            }\n        }\n    \"\"\"\n\n    self.midpoints = midpoints\n    self.bit_locations = bit_locations\n    self.exponent = exponent\n    self.starting_bit_weights = starting_bit_weights\n    self.truth_table = truth_table\n    self._validate_input_sequence_in_truth_table(truth_table)\n    self.domain = 2**self._get_depth_of_truth_table(truth_table)\n    self.flattened_truth_table = {k : None for k in range(self.domain)}\n    self._flatten_truth_table(truth_table)\n    self._gen_validity_mapping()\n    self.hparams = {\n        'midpoints': self.midpoints.tolist() if isinstance(self.midpoints, torch.Tensor) else self.midpoints,\n        'truth_table': self.truth_table,\n        'bit_locations_shape': list(self.bit_locations.shape),\n        'exponent': self.exponent,\n        'starting_bit_weights': self.starting_bit_weights.tolist() if self.starting_bit_weights is not None else None,\n        'domain': self.domain,\n        'name': self.__class__.__name__\n    }\n</code></pre>"},{"location":"reference/losses/#protocolopt.losses.LogicGateEndpointLossBase.compute_binary_error_rate","title":"<code>compute_binary_error_rate(microstate_paths)</code>","text":"<p>Compute the percentage of microstate paths ending in invalid states.</p> <p>Parameters:</p> Name Type Description Default <code>microstate_paths</code> <code>MicrostatePaths</code> <p>Microstate paths tensor (should be detached)               Shape: (Batch, Spatial_Dim, Time_Steps+1, 2)</p> required <p>Returns:</p> Type Description <code>float</code> <p>Float representing the error rate (0.0 to 1.0)</p> Source code in <code>src/protocolopt/losses/standard.py</code> <pre><code>def compute_binary_error_rate(self, microstate_paths: MicrostatePaths) -&gt; float:\n    \"\"\"\n    Compute the percentage of microstate paths ending in invalid states.\n\n    Args:\n        microstate_paths: Microstate paths tensor (should be detached)\n                          Shape: (Batch, Spatial_Dim, Time_Steps+1, 2)\n\n    Returns:\n        Float representing the error rate (0.0 to 1.0)\n    \"\"\"\n    starting_bits_int = self._compute_starting_bits_int(microstate_paths)\n    ending_bits_int = self._compute_ending_bits_int(microstate_paths)\n    valid_transitions = self.validity[starting_bits_int, ending_bits_int]\n    error_rate = (~valid_transitions).float().mean()\n    return error_rate.item()\n</code></pre>"},{"location":"reference/losses/#protocolopt.losses.LogicGateEndpointLossBase.compute_binary_trajectory_info","title":"<code>compute_binary_trajectory_info(microstate_paths)</code>","text":"<p>Compute starting and ending binary states for microstate paths.</p> <p>Parameters:</p> Name Type Description Default <code>microstate_paths</code> <code>MicrostatePaths</code> <p>Microstate paths tensor.               Shape: (Batch, Spatial_Dim, Time_Steps+1, 2)</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>Dictionary with 'starting_bits_int' and 'ending_bits_int' tensors</p> Source code in <code>src/protocolopt/losses/standard.py</code> <pre><code>def compute_binary_trajectory_info(self, microstate_paths: MicrostatePaths) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"\n    Compute starting and ending binary states for microstate paths.\n\n    Args:\n        microstate_paths: Microstate paths tensor.\n                          Shape: (Batch, Spatial_Dim, Time_Steps+1, 2)\n\n    Returns:\n        Dictionary with 'starting_bits_int' and 'ending_bits_int' tensors\n    \"\"\"\n    return {\n        'starting_bits_int': self._compute_starting_bits_int(microstate_paths),\n        'ending_bits_int': self._compute_ending_bits_int(microstate_paths)\n    }\n</code></pre>"},{"location":"reference/losses/#protocolopt.losses.StandardLogicGateLoss","title":"<code>StandardLogicGateLoss</code>","text":"<p>               Bases: <code>LogicGateEndpointLossBase</code></p> <p>Standard loss function combining logic gate endpoint error, work, variance, and smoothness.</p> Source code in <code>src/protocolopt/losses/standard.py</code> <pre><code>class StandardLogicGateLoss(LogicGateEndpointLossBase):\n    \"\"\"Standard loss function combining logic gate endpoint error, work, variance, and smoothness.\"\"\"\n\n    def __init__(self, midpoints, truth_table, bit_locations, endpoint_weight = 1, work_weight = 1, var_weight = 1, smoothness_weight = 1, exponent = 2, starting_bit_weights=None):\n        \"\"\"Initializes StandardLogicGateLoss.\n\n        Args:\n            midpoints: Bit boundaries.\n            truth_table: Valid transitions.\n            bit_locations: Target locations.\n            endpoint_weight: Weight for endpoint loss.\n            work_weight: Weight for work loss.\n            var_weight: Weight for variance loss.\n            smoothness_weight: Weight for smoothness penalty.\n            exponent: Exponent for distance.\n            starting_bit_weights: Weights for starting bits.\n        \"\"\"\n        super().__init__(midpoints, truth_table, bit_locations, exponent, starting_bit_weights)\n        self.endpoint_weight = endpoint_weight\n        self.work_weight = work_weight\n        self.var_weight = var_weight\n        self.smoothness_weight = smoothness_weight\n\n        self.hparams.update({\n            'endpoint_weight': self.endpoint_weight,\n            'work_weight': self.work_weight,\n            'var_weight': self.var_weight,\n            'smoothness_weight': self.smoothness_weight\n        })\n\n    def loss(self, potential_tensor: PotentialTensor, microstate_paths: MicrostatePaths, dw_tensor: WorkTensor, protocol_tensor: ControlSignal, dt: float) -&gt; torch.Tensor:\n        \"\"\"Computes the combined loss.\n\n        Args:\n            potential_tensor: Potential energy values. Shape: (Batch, Time_Steps)\n            microstate_paths: Microstate paths data. Shape: (Batch, Spatial_Dim, Time_Steps+1, 2)\n            dw_tensor: Change in potential energy at each step. Shape: (Batch, Time_Steps)\n            protocol_tensor: Control signals. Shape: (Control_Dim, Time_Steps)\n            dt: Time step size.\n\n        Returns:\n            Loss value. Shape: (Batch,)\n        \"\"\"\n        starting_bits_int = self._compute_starting_bits_int(microstate_paths)\n        endpoint_loss = self._endpoint_loss(microstate_paths)\n        work_loss_value = work_loss(dw_tensor)\n        var_loss_value = variance_loss(microstate_paths, starting_bits_int, self.domain)\n        smoothness_loss_value = temporal_smoothness_penalty(protocol_tensor, dt)\n        return (\n            self.endpoint_weight * endpoint_loss\n            + self.work_weight * work_loss_value\n            + self.var_weight * var_loss_value\n            + self.smoothness_weight * smoothness_loss_value\n        )\n\n    def log_components(self, potential_tensor: PotentialTensor, microstate_paths: MicrostatePaths, dw_tensor: WorkTensor, protocol_tensor: ControlSignal, dt: float) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"\n        Compute individual loss components for logging/analysis.\n\n        Args:\n            potential_tensor: Detached potential tensor\n            microstate_paths: Detached microstate paths tensor\n            dw_tensor: Detached work tensor\n            protocol_tensor: Protocol tensor\n            dt: Time step\n\n        Returns:\n            Dictionary with loss component values (already detached from input)\n        \"\"\"\n        starting_bits_int = self._compute_starting_bits_int(microstate_paths)\n        endpoint_loss_val = self._endpoint_loss(microstate_paths)\n        work_loss_val = work_loss(dw_tensor)\n        var_loss_val = variance_loss(microstate_paths, starting_bits_int, self.domain)\n        smoothness_loss_val = temporal_smoothness_penalty(protocol_tensor, dt)\n\n        return {\n            'endpoint_loss': endpoint_loss_val,\n            'work_loss': work_loss_val,\n            'variance_loss': var_loss_val,\n            'smoothness_loss': smoothness_loss_val\n        }\n</code></pre>"},{"location":"reference/losses/#protocolopt.losses.StandardLogicGateLoss.__init__","title":"<code>__init__(midpoints, truth_table, bit_locations, endpoint_weight=1, work_weight=1, var_weight=1, smoothness_weight=1, exponent=2, starting_bit_weights=None)</code>","text":"<p>Initializes StandardLogicGateLoss.</p> <p>Parameters:</p> Name Type Description Default <code>midpoints</code> <p>Bit boundaries.</p> required <code>truth_table</code> <p>Valid transitions.</p> required <code>bit_locations</code> <p>Target locations.</p> required <code>endpoint_weight</code> <p>Weight for endpoint loss.</p> <code>1</code> <code>work_weight</code> <p>Weight for work loss.</p> <code>1</code> <code>var_weight</code> <p>Weight for variance loss.</p> <code>1</code> <code>smoothness_weight</code> <p>Weight for smoothness penalty.</p> <code>1</code> <code>exponent</code> <p>Exponent for distance.</p> <code>2</code> <code>starting_bit_weights</code> <p>Weights for starting bits.</p> <code>None</code> Source code in <code>src/protocolopt/losses/standard.py</code> <pre><code>def __init__(self, midpoints, truth_table, bit_locations, endpoint_weight = 1, work_weight = 1, var_weight = 1, smoothness_weight = 1, exponent = 2, starting_bit_weights=None):\n    \"\"\"Initializes StandardLogicGateLoss.\n\n    Args:\n        midpoints: Bit boundaries.\n        truth_table: Valid transitions.\n        bit_locations: Target locations.\n        endpoint_weight: Weight for endpoint loss.\n        work_weight: Weight for work loss.\n        var_weight: Weight for variance loss.\n        smoothness_weight: Weight for smoothness penalty.\n        exponent: Exponent for distance.\n        starting_bit_weights: Weights for starting bits.\n    \"\"\"\n    super().__init__(midpoints, truth_table, bit_locations, exponent, starting_bit_weights)\n    self.endpoint_weight = endpoint_weight\n    self.work_weight = work_weight\n    self.var_weight = var_weight\n    self.smoothness_weight = smoothness_weight\n\n    self.hparams.update({\n        'endpoint_weight': self.endpoint_weight,\n        'work_weight': self.work_weight,\n        'var_weight': self.var_weight,\n        'smoothness_weight': self.smoothness_weight\n    })\n</code></pre>"},{"location":"reference/losses/#protocolopt.losses.StandardLogicGateLoss.log_components","title":"<code>log_components(potential_tensor, microstate_paths, dw_tensor, protocol_tensor, dt)</code>","text":"<p>Compute individual loss components for logging/analysis.</p> <p>Parameters:</p> Name Type Description Default <code>potential_tensor</code> <code>PotentialTensor</code> <p>Detached potential tensor</p> required <code>microstate_paths</code> <code>MicrostatePaths</code> <p>Detached microstate paths tensor</p> required <code>dw_tensor</code> <code>WorkTensor</code> <p>Detached work tensor</p> required <code>protocol_tensor</code> <code>ControlSignal</code> <p>Protocol tensor</p> required <code>dt</code> <code>float</code> <p>Time step</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>Dictionary with loss component values (already detached from input)</p> Source code in <code>src/protocolopt/losses/standard.py</code> <pre><code>def log_components(self, potential_tensor: PotentialTensor, microstate_paths: MicrostatePaths, dw_tensor: WorkTensor, protocol_tensor: ControlSignal, dt: float) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"\n    Compute individual loss components for logging/analysis.\n\n    Args:\n        potential_tensor: Detached potential tensor\n        microstate_paths: Detached microstate paths tensor\n        dw_tensor: Detached work tensor\n        protocol_tensor: Protocol tensor\n        dt: Time step\n\n    Returns:\n        Dictionary with loss component values (already detached from input)\n    \"\"\"\n    starting_bits_int = self._compute_starting_bits_int(microstate_paths)\n    endpoint_loss_val = self._endpoint_loss(microstate_paths)\n    work_loss_val = work_loss(dw_tensor)\n    var_loss_val = variance_loss(microstate_paths, starting_bits_int, self.domain)\n    smoothness_loss_val = temporal_smoothness_penalty(protocol_tensor, dt)\n\n    return {\n        'endpoint_loss': endpoint_loss_val,\n        'work_loss': work_loss_val,\n        'variance_loss': var_loss_val,\n        'smoothness_loss': smoothness_loss_val\n    }\n</code></pre>"},{"location":"reference/losses/#protocolopt.losses.StandardLogicGateLoss.loss","title":"<code>loss(potential_tensor, microstate_paths, dw_tensor, protocol_tensor, dt)</code>","text":"<p>Computes the combined loss.</p> <p>Parameters:</p> Name Type Description Default <code>potential_tensor</code> <code>PotentialTensor</code> <p>Potential energy values. Shape: (Batch, Time_Steps)</p> required <code>microstate_paths</code> <code>MicrostatePaths</code> <p>Microstate paths data. Shape: (Batch, Spatial_Dim, Time_Steps+1, 2)</p> required <code>dw_tensor</code> <code>WorkTensor</code> <p>Change in potential energy at each step. Shape: (Batch, Time_Steps)</p> required <code>protocol_tensor</code> <code>ControlSignal</code> <p>Control signals. Shape: (Control_Dim, Time_Steps)</p> required <code>dt</code> <code>float</code> <p>Time step size.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Loss value. Shape: (Batch,)</p> Source code in <code>src/protocolopt/losses/standard.py</code> <pre><code>def loss(self, potential_tensor: PotentialTensor, microstate_paths: MicrostatePaths, dw_tensor: WorkTensor, protocol_tensor: ControlSignal, dt: float) -&gt; torch.Tensor:\n    \"\"\"Computes the combined loss.\n\n    Args:\n        potential_tensor: Potential energy values. Shape: (Batch, Time_Steps)\n        microstate_paths: Microstate paths data. Shape: (Batch, Spatial_Dim, Time_Steps+1, 2)\n        dw_tensor: Change in potential energy at each step. Shape: (Batch, Time_Steps)\n        protocol_tensor: Control signals. Shape: (Control_Dim, Time_Steps)\n        dt: Time step size.\n\n    Returns:\n        Loss value. Shape: (Batch,)\n    \"\"\"\n    starting_bits_int = self._compute_starting_bits_int(microstate_paths)\n    endpoint_loss = self._endpoint_loss(microstate_paths)\n    work_loss_value = work_loss(dw_tensor)\n    var_loss_value = variance_loss(microstate_paths, starting_bits_int, self.domain)\n    smoothness_loss_value = temporal_smoothness_penalty(protocol_tensor, dt)\n    return (\n        self.endpoint_weight * endpoint_loss\n        + self.work_weight * work_loss_value\n        + self.var_weight * var_loss_value\n        + self.smoothness_weight * smoothness_loss_value\n    )\n</code></pre>"},{"location":"reference/losses/#protocolopt.losses.temporal_smoothness_penalty","title":"<code>temporal_smoothness_penalty(protocol_tensor, dt)</code>","text":"<p>Calculates the mean squared time-derivative of control parameters.</p> <p>Parameters:</p> Name Type Description Default <code>protocol_tensor</code> <p>Time-dependent control signals.</p> required <code>dt</code> <p>Time step size.</p> required <p>Returns:</p> Type Description <p>Mean squared derivative penalty.</p> Source code in <code>src/protocolopt/losses/functional.py</code> <pre><code>def temporal_smoothness_penalty(protocol_tensor, dt):\n    \"\"\"Calculates the mean squared time-derivative of control parameters.\n\n    Args:\n        protocol_tensor: Time-dependent control signals.\n        dt: Time step size.\n\n    Returns:\n        Mean squared derivative penalty.\n    \"\"\"\n    dcoeff_dt = (protocol_tensor[:, 1:] - protocol_tensor[:, :-1]) / dt\n    return (dcoeff_dt ** 2).mean()\n</code></pre>"},{"location":"reference/losses/#protocolopt.losses.variance_loss","title":"<code>variance_loss(microstate_paths, starting_bits_int, domain_size, phase_dimension=0)</code>","text":"<p>Computes the variance of microstate paths that started together.</p> <p>Parameters:</p> Name Type Description Default <code>microstate_paths</code> <code>MicrostatePaths</code> <p>Microstate path data.                Shape: (Batch, Spatial_Dim, Time_Steps+1, 2)                Dimension 3 is (position, velocity).</p> required <code>starting_bits_int</code> <code>Tensor</code> <p>Starting bit states.                Shape: (Batch,)</p> required <code>domain_size</code> <code>int</code> <p>Number of possible starting states (Decimal range of bitstring).</p> required <code>phase_dimension</code> <code>int</code> <p>0 for position, 1 for velocity.</p> <code>0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Variance loss. Shape: (Batch,)</p> Source code in <code>src/protocolopt/losses/functional.py</code> <pre><code>def variance_loss(microstate_paths: MicrostatePaths, starting_bits_int: torch.Tensor, domain_size: int, phase_dimension: int = 0) -&gt; torch.Tensor:\n    \"\"\"Computes the variance of microstate paths that started together.\n\n    Args:\n        microstate_paths: Microstate path data.\n                           Shape: (Batch, Spatial_Dim, Time_Steps+1, 2)\n                           Dimension 3 is (position, velocity).\n        starting_bits_int: Starting bit states.\n                           Shape: (Batch,)\n        domain_size: Number of possible starting states (Decimal range of bitstring).\n        phase_dimension: 0 for position, 1 for velocity.\n\n    Returns:\n        Variance loss. Shape: (Batch,)\n    \"\"\"\n    #microstate_paths is of shape (num_samples, spatial_dimensions, time_steps+1, 2 (position and velocity))\n    #starting_bits_int is of shape (num_samples,)\n    #domain_size is the number of possible starting bits\n    #phase_dimension is the dimension of the phase to compute the variance of, allowing for seperate position and velocity loss computations\n    var_loss = torch.zeros(microstate_paths.shape[0], device=microstate_paths.device)\n    for i in range(domain_size):\n        mask = starting_bits_int == i\n        if mask.any():\n            var_loss[mask] = ((microstate_paths[mask, :, :, phase_dimension] -\n                   microstate_paths[mask, :, :, phase_dimension].mean(axis=0, keepdim=True))**2\n                  ).mean(dim=(1, 2)) #compute the variance using cohort mean over space and time then mean each trajectories var over space and time\n    return var_loss\n</code></pre>"},{"location":"reference/losses/#protocolopt.losses.work_loss","title":"<code>work_loss(dw_tensor)</code>","text":"<p>Calculates the total work from the discrete work increments.</p> <p>Parameters:</p> Name Type Description Default <code>dw_tensor</code> <code>WorkTensor</code> <p>Change in potential energy at each step.        Shape: (Batch, Time_Steps)</p> required <p>Returns:</p> Type Description <p>Total work done along each path. Shape: (Batch,)</p> Source code in <code>src/protocolopt/losses/functional.py</code> <pre><code>def work_loss(dw_tensor: WorkTensor):\n    \"\"\"Calculates the total work from the discrete work increments.\n\n    Args:\n        dw_tensor: Change in potential energy at each step.\n                   Shape: (Batch, Time_Steps)\n\n    Returns:\n        Total work done along each path. Shape: (Batch,)\n    \"\"\"\n    return dw_tensor.sum(axis=-1)\n</code></pre>"},{"location":"reference/potentials/","title":"Potentials","text":""},{"location":"reference/potentials/#protocolopt.potentials.GeneralCoupledPotential","title":"<code>GeneralCoupledPotential</code>","text":"<p>               Bases: <code>Potential</code></p> <p>A generalized coupled potential including quartic, quadratic, linear, and interaction terms.</p> <p>The potential is given by: V(x) = sum(a_i * x_i^4 - b_i * x_i^2 + c_i * x_i) + sum(mix_ij * x_i * x_j)</p> Source code in <code>src/protocolopt/potentials/coupled.py</code> <pre><code>class GeneralCoupledPotential(Potential):\n    \"\"\"A generalized coupled potential including quartic, quadratic, linear, and interaction terms.\n\n    The potential is given by:\n    V(x) = sum(a_i * x_i^4 - b_i * x_i^2 + c_i * x_i) + sum(mix_ij * x_i * x_j)\n    \"\"\"\n\n    def __init__(self, spatial_dimensions: int, has_c: bool = True, has_mix: bool = True, compile_mode: bool = True) -&gt; None:\n        \"\"\"Initializes the GeneralCoupledPotential.\n\n        Args:\n            spatial_dimensions: Number of spatial dimensions.\n            has_c: Whether to include linear terms (c_i * x_i).\n            has_mix: Whether to include interaction terms (mix_ij * x_i * x_j).\n            compile_mode: Whether to compile gradient functions.\n        \"\"\"\n        super().__init__(compile_mode)\n        self.spatial_dim = spatial_dimensions\n        self.has_c = has_c\n        self.has_mix = has_mix\n        if self.spatial_dim == 1:\n            self.has_mix = False\n\n        self.triu_indices = torch.triu_indices(row=spatial_dimensions, col=spatial_dimensions, offset=1)\n\n        self.hparams = {\n            'spatial_dim': self.spatial_dim,\n            'has_c': self.has_c,\n            'has_mix': self.has_mix,\n            'compile_mode': self.compile_mode,\n            'name': self.__class__.__name__\n        }\n\n    def potential_value(self, space_grid: StateSpace, protocol_tensor: ControlVector) -&gt; torch.Tensor:\n        \"\"\"Computes the coupled potential value.\n\n        Args:\n            space_grid: Spatial coordinates.\n                        Shape: (Batch, N) or (N,)\n            protocol_tensor: Control vector. Layout: [N Quartics, N Quadratics, N Linears (optional), K Interactions (optional)].\n                             Shape: (Total_Coeffs,)\n\n        Returns:\n            Potential value. Shape: (Batch,) or scalar.\n        \"\"\"\n        # space_grid: (Batch, N) or (N,)\n        # protocol_tensor: (Total_Coeffs)\n        is_unbatched = space_grid.ndim == 1\n        if is_unbatched:\n            space_grid = space_grid.unsqueeze(0)\n        # assume layout: [N Quartics, N Quadratics, N Linears, K Interactions]\n        N = self.spatial_dim\n\n        current_idx = 0\n\n        # Quartic terms (a)\n        a = protocol_tensor[current_idx : current_idx + N]\n        current_idx += N\n\n        # Quadratic terms (b)\n        b = protocol_tensor[current_idx : current_idx + N]\n        current_idx += N\n\n        # Linear terms (c)\n        if self.has_c:\n            c = protocol_tensor[current_idx : current_idx + N]\n            current_idx += N\n        else:\n            c = torch.zeros_like(a)\n\n        # Interaction terms\n        if self.has_mix:\n            mix = protocol_tensor[current_idx : ]\n        else:\n            mix = None\n\n        V_independent = torch.sum(a * space_grid**4 - b * space_grid**2 + c * space_grid, dim=-1)\n\n        if N &gt; 1 and self.has_mix:\n            # grabs every unique pair for interaction\n            vals_i = space_grid[:, self.triu_indices[0]]\n            vals_j = space_grid[:, self.triu_indices[1]]\n\n            V_interaction = torch.sum(mix * vals_i * vals_j, dim=-1)\n        else:\n            V_interaction = 0.0 #backwards compatibility\n        result = V_independent + V_interaction\n\n        if is_unbatched:\n            return result.squeeze(0)\n        return result\n</code></pre>"},{"location":"reference/potentials/#protocolopt.potentials.GeneralCoupledPotential.__init__","title":"<code>__init__(spatial_dimensions, has_c=True, has_mix=True, compile_mode=True)</code>","text":"<p>Initializes the GeneralCoupledPotential.</p> <p>Parameters:</p> Name Type Description Default <code>spatial_dimensions</code> <code>int</code> <p>Number of spatial dimensions.</p> required <code>has_c</code> <code>bool</code> <p>Whether to include linear terms (c_i * x_i).</p> <code>True</code> <code>has_mix</code> <code>bool</code> <p>Whether to include interaction terms (mix_ij * x_i * x_j).</p> <code>True</code> <code>compile_mode</code> <code>bool</code> <p>Whether to compile gradient functions.</p> <code>True</code> Source code in <code>src/protocolopt/potentials/coupled.py</code> <pre><code>def __init__(self, spatial_dimensions: int, has_c: bool = True, has_mix: bool = True, compile_mode: bool = True) -&gt; None:\n    \"\"\"Initializes the GeneralCoupledPotential.\n\n    Args:\n        spatial_dimensions: Number of spatial dimensions.\n        has_c: Whether to include linear terms (c_i * x_i).\n        has_mix: Whether to include interaction terms (mix_ij * x_i * x_j).\n        compile_mode: Whether to compile gradient functions.\n    \"\"\"\n    super().__init__(compile_mode)\n    self.spatial_dim = spatial_dimensions\n    self.has_c = has_c\n    self.has_mix = has_mix\n    if self.spatial_dim == 1:\n        self.has_mix = False\n\n    self.triu_indices = torch.triu_indices(row=spatial_dimensions, col=spatial_dimensions, offset=1)\n\n    self.hparams = {\n        'spatial_dim': self.spatial_dim,\n        'has_c': self.has_c,\n        'has_mix': self.has_mix,\n        'compile_mode': self.compile_mode,\n        'name': self.__class__.__name__\n    }\n</code></pre>"},{"location":"reference/potentials/#protocolopt.potentials.GeneralCoupledPotential.potential_value","title":"<code>potential_value(space_grid, protocol_tensor)</code>","text":"<p>Computes the coupled potential value.</p> <p>Parameters:</p> Name Type Description Default <code>space_grid</code> <code>StateSpace</code> <p>Spatial coordinates.         Shape: (Batch, N) or (N,)</p> required <code>protocol_tensor</code> <code>ControlVector</code> <p>Control vector. Layout: [N Quartics, N Quadratics, N Linears (optional), K Interactions (optional)].              Shape: (Total_Coeffs,)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Potential value. Shape: (Batch,) or scalar.</p> Source code in <code>src/protocolopt/potentials/coupled.py</code> <pre><code>def potential_value(self, space_grid: StateSpace, protocol_tensor: ControlVector) -&gt; torch.Tensor:\n    \"\"\"Computes the coupled potential value.\n\n    Args:\n        space_grid: Spatial coordinates.\n                    Shape: (Batch, N) or (N,)\n        protocol_tensor: Control vector. Layout: [N Quartics, N Quadratics, N Linears (optional), K Interactions (optional)].\n                         Shape: (Total_Coeffs,)\n\n    Returns:\n        Potential value. Shape: (Batch,) or scalar.\n    \"\"\"\n    # space_grid: (Batch, N) or (N,)\n    # protocol_tensor: (Total_Coeffs)\n    is_unbatched = space_grid.ndim == 1\n    if is_unbatched:\n        space_grid = space_grid.unsqueeze(0)\n    # assume layout: [N Quartics, N Quadratics, N Linears, K Interactions]\n    N = self.spatial_dim\n\n    current_idx = 0\n\n    # Quartic terms (a)\n    a = protocol_tensor[current_idx : current_idx + N]\n    current_idx += N\n\n    # Quadratic terms (b)\n    b = protocol_tensor[current_idx : current_idx + N]\n    current_idx += N\n\n    # Linear terms (c)\n    if self.has_c:\n        c = protocol_tensor[current_idx : current_idx + N]\n        current_idx += N\n    else:\n        c = torch.zeros_like(a)\n\n    # Interaction terms\n    if self.has_mix:\n        mix = protocol_tensor[current_idx : ]\n    else:\n        mix = None\n\n    V_independent = torch.sum(a * space_grid**4 - b * space_grid**2 + c * space_grid, dim=-1)\n\n    if N &gt; 1 and self.has_mix:\n        # grabs every unique pair for interaction\n        vals_i = space_grid[:, self.triu_indices[0]]\n        vals_j = space_grid[:, self.triu_indices[1]]\n\n        V_interaction = torch.sum(mix * vals_i * vals_j, dim=-1)\n    else:\n        V_interaction = 0.0 #backwards compatibility\n    result = V_independent + V_interaction\n\n    if is_unbatched:\n        return result.squeeze(0)\n    return result\n</code></pre>"},{"location":"reference/potentials/#protocolopt.potentials.QuarticPotential","title":"<code>QuarticPotential</code>","text":"<p>               Bases: <code>Potential</code></p> <p>Potential of form V(x, t) = a(t)x^4 - b(t)x^2.</p> Source code in <code>src/protocolopt/potentials/quartic.py</code> <pre><code>class QuarticPotential(Potential):\n    \"\"\"Potential of form V(x, t) = a(t)x^4 - b(t)x^2.\"\"\"\n\n    def __init__(self, compile_mode: bool = True):\n        super().__init__(compile_mode)\n        self.hparams = {\n            'name': self.__class__.__name__,\n            'compile_mode': self.compile_mode\n        }\n\n    def potential_value(self, space_grid: StateSpace, protocol_tensor: ControlVector) -&gt; torch.Tensor:\n        \"\"\"Computes the quartic potential value.\n\n        Args:\n            space_grid: Spatial coordinates.\n                        Shape: (Batch, Spatial_Dim) or (Spatial_Dim,)\n            protocol_tensor: Control vector [a, b].\n                             Shape: (Control_Dim,)\n\n        Returns:\n            Potential value.\n        \"\"\"\n        return torch.sum(protocol_tensor[0] * space_grid**4 - protocol_tensor[1] * space_grid**2, dim=-1)\n</code></pre>"},{"location":"reference/potentials/#protocolopt.potentials.QuarticPotential.potential_value","title":"<code>potential_value(space_grid, protocol_tensor)</code>","text":"<p>Computes the quartic potential value.</p> <p>Parameters:</p> Name Type Description Default <code>space_grid</code> <code>StateSpace</code> <p>Spatial coordinates.         Shape: (Batch, Spatial_Dim) or (Spatial_Dim,)</p> required <code>protocol_tensor</code> <code>ControlVector</code> <p>Control vector [a, b].              Shape: (Control_Dim,)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Potential value.</p> Source code in <code>src/protocolopt/potentials/quartic.py</code> <pre><code>def potential_value(self, space_grid: StateSpace, protocol_tensor: ControlVector) -&gt; torch.Tensor:\n    \"\"\"Computes the quartic potential value.\n\n    Args:\n        space_grid: Spatial coordinates.\n                    Shape: (Batch, Spatial_Dim) or (Spatial_Dim,)\n        protocol_tensor: Control vector [a, b].\n                         Shape: (Control_Dim,)\n\n    Returns:\n        Potential value.\n    \"\"\"\n    return torch.sum(protocol_tensor[0] * space_grid**4 - protocol_tensor[1] * space_grid**2, dim=-1)\n</code></pre>"},{"location":"reference/potentials/#protocolopt.potentials.QuarticPotentialWithLinearTerm","title":"<code>QuarticPotentialWithLinearTerm</code>","text":"<p>               Bases: <code>Potential</code></p> <p>Potential of form V(x, t) = a(t)x^4 - b(t)x^2 + c(t)x.</p> Source code in <code>src/protocolopt/potentials/quartic.py</code> <pre><code>class QuarticPotentialWithLinearTerm(Potential):\n    \"\"\"Potential of form V(x, t) = a(t)x^4 - b(t)x^2 + c(t)x.\"\"\"\n\n    def __init__(self, compile_mode: bool = True):\n        super().__init__(compile_mode)\n        self.hparams = {\n            'name': self.__class__.__name__,\n            'compile_mode': self.compile_mode\n        }\n\n    def potential_value(self, space_grid: StateSpace, protocol_tensor: ControlVector) -&gt; torch.Tensor:\n        \"\"\"Computes the quartic potential with linear term.\n\n        Args:\n            space_grid: Spatial coordinates.\n                        Shape: (Batch, Spatial_Dim) or (Spatial_Dim,)\n            protocol_tensor: Control vector [a, b, c].\n                             Shape: (Control_Dim,)\n\n        Returns:\n            Potential value.\n        \"\"\"\n        return torch.sum(protocol_tensor[0] * space_grid**4 - protocol_tensor[1] * space_grid**2 + protocol_tensor[2] * space_grid, dim=-1)\n</code></pre>"},{"location":"reference/potentials/#protocolopt.potentials.QuarticPotentialWithLinearTerm.potential_value","title":"<code>potential_value(space_grid, protocol_tensor)</code>","text":"<p>Computes the quartic potential with linear term.</p> <p>Parameters:</p> Name Type Description Default <code>space_grid</code> <code>StateSpace</code> <p>Spatial coordinates.         Shape: (Batch, Spatial_Dim) or (Spatial_Dim,)</p> required <code>protocol_tensor</code> <code>ControlVector</code> <p>Control vector [a, b, c].              Shape: (Control_Dim,)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Potential value.</p> Source code in <code>src/protocolopt/potentials/quartic.py</code> <pre><code>def potential_value(self, space_grid: StateSpace, protocol_tensor: ControlVector) -&gt; torch.Tensor:\n    \"\"\"Computes the quartic potential with linear term.\n\n    Args:\n        space_grid: Spatial coordinates.\n                    Shape: (Batch, Spatial_Dim) or (Spatial_Dim,)\n        protocol_tensor: Control vector [a, b, c].\n                         Shape: (Control_Dim,)\n\n    Returns:\n        Potential value.\n    \"\"\"\n    return torch.sum(protocol_tensor[0] * space_grid**4 - protocol_tensor[1] * space_grid**2 + protocol_tensor[2] * space_grid, dim=-1)\n</code></pre>"},{"location":"reference/protocols/","title":"Protocols","text":""},{"location":"reference/protocols/#protocolopt.protocols.LinearPiecewise","title":"<code>LinearPiecewise</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol parameterized by linear interpolation between knots.</p> Source code in <code>src/protocolopt/protocols/piecewise.py</code> <pre><code>class LinearPiecewise(Protocol):\n    \"\"\"Protocol parameterized by linear interpolation between knots.\"\"\"\n\n    def __init__(self, control_dim: int, time_steps: int, knot_count: int, initial_coeff_guess: torch.Tensor, endpoints: Optional[torch.Tensor] = None) -&gt; None:\n        \"\"\"Initializes the LinearPiecewise protocol.\n\n        Args:\n            control_dim: Number of coefficients to model.\n            time_steps: Number of time steps for interpolation.\n            knot_count: Total number of knots (including endpoints).\n            initial_coeff_guess: Initial guess for the trainable knots.\n            endpoints: Fixed values for the start and end knots. Shape: (Coeffs, 2).\n\n        Raises:\n            ValueError: If initial guess shape is inconsistent with knot count.\n        \"\"\"\n        if endpoints is not None:\n            fixed_starting = True\n        else:\n            fixed_starting = False\n        super().__init__(time_steps, fixed_starting)\n        if initial_coeff_guess.shape != (control_dim, knot_count if endpoints is None else knot_count - 2):\n            raise ValueError(f\"Initial coefficient guess must be of shape (control_dim, knot_count if endpoints is None else knot_count - 2), got {initial_coeff_guess.shape}\")\n\n        self.control_dim = control_dim\n        self.knot_count = knot_count\n        self.endpoints = endpoints\n        if self.endpoints is not None:\n            self._trainable_params = torch.nn.Parameter(initial_coeff_guess.clone())\n        else:\n            self._trainable_params = torch.nn.Parameter(initial_coeff_guess.clone())\n\n        self.device = initial_coeff_guess.device\n\n        self.hparams = {\n            'control_dim': self.control_dim,\n            'time_steps': self.time_steps,\n            'knot_count': self.knot_count,\n            'fixed_starting': self.fixed_starting,\n            'endpoints_shape': list(self.endpoints.shape) if self.endpoints is not None else None,\n            'name': self.__class__.__name__\n        }\n\n    def _get_knots(self) -&gt; torch.Tensor:\n        if self.endpoints is not None:\n            return torch.cat([self.endpoints[..., 0:1], self._trainable_params, self.endpoints[..., -1:]], dim=-1)\n        else:\n            return self.trainable_params\n\n    def get_protocol_tensor(self) -&gt; torch.Tensor:\n        \"\"\"Interpolates knots to get the full coefficient grid.\n\n        Returns:\n            Coefficient grid. Shape: (Control_Dim, Time_Steps+1).\n        \"\"\"\n        knots = self._get_knots()\n        protocol_tensor = F.interpolate(knots.unsqueeze(0), size=self.time_steps + 1, mode='linear', align_corners=True)\n        return protocol_tensor.squeeze(0)\n\n    def trainable_params(self) -&gt; List[torch.nn.Parameter]:\n        \"\"\"Returns the trainable knots.\"\"\"\n        return [self._trainable_params]\n</code></pre>"},{"location":"reference/protocols/#protocolopt.protocols.LinearPiecewise.__init__","title":"<code>__init__(control_dim, time_steps, knot_count, initial_coeff_guess, endpoints=None)</code>","text":"<p>Initializes the LinearPiecewise protocol.</p> <p>Parameters:</p> Name Type Description Default <code>control_dim</code> <code>int</code> <p>Number of coefficients to model.</p> required <code>time_steps</code> <code>int</code> <p>Number of time steps for interpolation.</p> required <code>knot_count</code> <code>int</code> <p>Total number of knots (including endpoints).</p> required <code>initial_coeff_guess</code> <code>Tensor</code> <p>Initial guess for the trainable knots.</p> required <code>endpoints</code> <code>Optional[Tensor]</code> <p>Fixed values for the start and end knots. Shape: (Coeffs, 2).</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If initial guess shape is inconsistent with knot count.</p> Source code in <code>src/protocolopt/protocols/piecewise.py</code> <pre><code>def __init__(self, control_dim: int, time_steps: int, knot_count: int, initial_coeff_guess: torch.Tensor, endpoints: Optional[torch.Tensor] = None) -&gt; None:\n    \"\"\"Initializes the LinearPiecewise protocol.\n\n    Args:\n        control_dim: Number of coefficients to model.\n        time_steps: Number of time steps for interpolation.\n        knot_count: Total number of knots (including endpoints).\n        initial_coeff_guess: Initial guess for the trainable knots.\n        endpoints: Fixed values for the start and end knots. Shape: (Coeffs, 2).\n\n    Raises:\n        ValueError: If initial guess shape is inconsistent with knot count.\n    \"\"\"\n    if endpoints is not None:\n        fixed_starting = True\n    else:\n        fixed_starting = False\n    super().__init__(time_steps, fixed_starting)\n    if initial_coeff_guess.shape != (control_dim, knot_count if endpoints is None else knot_count - 2):\n        raise ValueError(f\"Initial coefficient guess must be of shape (control_dim, knot_count if endpoints is None else knot_count - 2), got {initial_coeff_guess.shape}\")\n\n    self.control_dim = control_dim\n    self.knot_count = knot_count\n    self.endpoints = endpoints\n    if self.endpoints is not None:\n        self._trainable_params = torch.nn.Parameter(initial_coeff_guess.clone())\n    else:\n        self._trainable_params = torch.nn.Parameter(initial_coeff_guess.clone())\n\n    self.device = initial_coeff_guess.device\n\n    self.hparams = {\n        'control_dim': self.control_dim,\n        'time_steps': self.time_steps,\n        'knot_count': self.knot_count,\n        'fixed_starting': self.fixed_starting,\n        'endpoints_shape': list(self.endpoints.shape) if self.endpoints is not None else None,\n        'name': self.__class__.__name__\n    }\n</code></pre>"},{"location":"reference/protocols/#protocolopt.protocols.LinearPiecewise.get_protocol_tensor","title":"<code>get_protocol_tensor()</code>","text":"<p>Interpolates knots to get the full coefficient grid.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Coefficient grid. Shape: (Control_Dim, Time_Steps+1).</p> Source code in <code>src/protocolopt/protocols/piecewise.py</code> <pre><code>def get_protocol_tensor(self) -&gt; torch.Tensor:\n    \"\"\"Interpolates knots to get the full coefficient grid.\n\n    Returns:\n        Coefficient grid. Shape: (Control_Dim, Time_Steps+1).\n    \"\"\"\n    knots = self._get_knots()\n    protocol_tensor = F.interpolate(knots.unsqueeze(0), size=self.time_steps + 1, mode='linear', align_corners=True)\n    return protocol_tensor.squeeze(0)\n</code></pre>"},{"location":"reference/protocols/#protocolopt.protocols.LinearPiecewise.trainable_params","title":"<code>trainable_params()</code>","text":"<p>Returns the trainable knots.</p> Source code in <code>src/protocolopt/protocols/piecewise.py</code> <pre><code>def trainable_params(self) -&gt; List[torch.nn.Parameter]:\n    \"\"\"Returns the trainable knots.\"\"\"\n    return [self._trainable_params]\n</code></pre>"},{"location":"reference/sampling/","title":"Sampling","text":""},{"location":"reference/sampling/#protocolopt.sampling.ConditionalFlow","title":"<code>ConditionalFlow</code>","text":"<p>               Bases: <code>McmcNuts</code>, <code>Module</code></p> <p>Initial condition generator using a Conditional Normalizing Flow trained on MCMC samples.</p> Source code in <code>src/protocolopt/sampling/flow.py</code> <pre><code>class ConditionalFlow(McmcNuts, nn.Module):\n    \"\"\"Initial condition generator using a Conditional Normalizing Flow trained on MCMC samples.\"\"\"\n\n    def __init__(\n        self,\n        dt: float,\n        gamma: float,\n        mass: float,\n        device: torch.device,\n        spatial_dimensions: int = 1,\n        time_steps: int = 1000,\n        beta: float = 1.0,\n        starting_bounds: Optional[torch.Tensor] = None,\n        samples_per_well: Optional[int] = None,\n        num_samples: int = 5000,\n        chains_per_well: int = 1,\n        warmup_ratio: float = 0.1,\n        min_neff: Optional[float] = None,\n        run_every_epoch: bool = False,\n        flow_layers: int = 4,\n        flow_epochs: int = 300,\n        flow_batch_size: int = 256,\n        flow_training_samples_per_well: int = 500\n    ) -&gt; None:\n        \"\"\"Initializes the ConditionalFlow generator.\n\n        Args:\n            dt (float): Time step.\n            gamma (float): Friction coefficient.\n            mass (float): Particle mass.\n            device (torch.device): Torch device.\n            spatial_dimensions (int): Number of spatial dimensions.\n            time_steps (int): Number of time steps.\n            beta (float): 1/kT\n            starting_bounds (Optional[torch.Tensor]): Bounds for starting positions. Defaults to global bounds if None.\n            samples_per_well (int): Samples per well.\n            num_samples (int): Total samples if not per well.\n            chains_per_well (int): Chains per well.\n            warmup_ratio (float): Ratio of warmup steps.\n            min_neff (float): Minimum effective sample size.\n            run_every_epoch (bool): Whether to run sampling every epoch.\n            flow_layers (int): Number of flow layers.\n            flow_epochs (int): Number of training epochs for the flow.\n            flow_batch_size (int): Batch size for flow training.\n            flow_training_samples_per_well (int): Samples per well for training the flow.\n        \"\"\"\n        nn.Module.__init__(self)\n        super().__init__(\n            dt=dt,\n            gamma=gamma,\n            mass=mass,\n            device=device,\n            spatial_dimensions=spatial_dimensions,\n            time_steps=time_steps,\n            beta=beta,\n            starting_bounds=starting_bounds,\n            samples_per_well=samples_per_well,\n            num_samples=num_samples,\n            chains_per_well=chains_per_well,\n            warmup_ratio=warmup_ratio,\n            min_neff=min_neff,\n            run_every_epoch=run_every_epoch\n        )\n\n        if self.spatial_dimensions &gt; 8:\n            print('WARNING: when spatial dimensions are greater than 8 the number of samples is forced to 8 * samples_per_well and is taken randomly from the global bounds. You are NOT guaranteed to have samples from each well each epoch nor the normalizing flow to properly learn the entire bitstring space.')\n            self.force_random = True\n        else:\n            self.force_random = False\n\n        self.context_dim = self.spatial_dimensions\n\n        self.original_bounds = self.starting_bounds.clone()\n\n        transforms = []\n        if flow_layers &lt; 2:\n            raise ValueError(\"Flow layers must be at least 2 and at least 4 is highly recommended\")\n        for _ in range(flow_layers):\n            c1 = T.conditional_spline(\n                self.spatial_dimensions,\n                context_dim=self.context_dim, # number of bits in the bitstring\n                count_bins=16, #number of bins in the spline\n                bound=3.0 #since we are flowing from a N(0, 1) this is ~99.7% of the distribution\n            ).to(device)\n\n            transforms.append(c1)\n            transforms.append(T.Permute(torch.randperm(self.spatial_dimensions, device=device)))\n\n        self.base_dist = dist.Normal(torch.zeros(self.spatial_dimensions, device=device),\n                                     torch.ones(self.spatial_dimensions, device=device))\n\n        self.flow_dist = dist.ConditionalTransformedDistribution(self.base_dist, transforms)\n        self.flow_modules = nn.ModuleList([t for t in transforms if isinstance(t, nn.Module)])\n\n        self.is_trained = False\n\n        # saving the mean and std for standardization in the model for saving and loading\n        self.register_buffer('data_mean', torch.zeros(self.spatial_dimensions))\n        self.register_buffer('data_std', torch.ones(self.spatial_dimensions))\n\n        self.flow_epochs = flow_epochs\n        self.flow_batch_size = flow_batch_size\n        self.flow_training_well_count = 2**self.spatial_dimensions\n        self.flow_training_samples_per_well = flow_training_samples_per_well\n\n        self.hparams.update({\n            'force_random': self.force_random,\n            'context_dim': self.context_dim,\n            'flow_epochs': self.flow_epochs,\n            'flow_batch_size': self.flow_batch_size,\n            'flow_training_well_count': self.flow_training_well_count,\n            'flow_training_samples_per_well': self.flow_training_samples_per_well\n        })\n\n    def set_bounds_from_bits(self, target_bitstring: torch.Tensor, loss: Loss) -&gt; None:\n        \"\"\"Updates sampling bounds to target a specific bitstring well.\n\n        Args:\n            target_bitstring: Tensor representing the target bitstring (e.g., [0, 1]).\n            loss: Loss object containing midpoint information.\n        \"\"\"\n        if not hasattr(loss, 'midpoints'):\n            raise RuntimeError(\"Loss object must have .midpoints to define wells.\")\n\n        midpoints = loss.midpoints\n        global_bounds = self.original_bounds\n\n        new_bounds = []\n\n        for dim_idx in range(self.spatial_dimensions):\n            bit = target_bitstring[dim_idx]\n            low, high = global_bounds[dim_idx]\n            mid = midpoints[dim_idx]\n\n            if bit == 0:\n                new_bounds.append([low, mid])\n            elif bit == 1:\n                new_bounds.append([mid, high])\n            else:\n                raise ValueError(f\"Bit must be 0 or 1, got {bit}\")\n\n        # update the bounds used by the parent class\n        self.starting_bounds = torch.tensor(new_bounds, device=self.device)\n\n        # runs the parent class in global mode on this subset, basically redoing the per well logic but packed in a way the flow model needs to learn from\n        self.samples_per_well = None\n\n    def _train_flow(self, potential: Potential, protocol: Protocol, loss: Loss) -&gt; None:\n        \"\"\"Trains the normalizing flow on MCMC samples.\"\"\"\n        print(f\"Generating training data for {self.flow_training_well_count} random wells using inherited NUTS...\")\n\n        # save the original number of samples and bounds\n        original_samples_per_well = self.samples_per_well\n        original_bounds_backup = self.starting_bounds.clone()\n        original_num_samples = self.num_samples\n\n        # Set parameters for training data generation\n        # We use the user-specified flow_training_samples_per_well\n        # Note: _run_multichain_mcmc uses mcmc_num_samples internally when samples_per_well is None\n        # But here we are simulating \"global\" sampling for specific wells by setting bounds manually\n        self.num_samples = self.flow_training_samples_per_well * self.chains_per_well\n\n        try:\n            all_samples = []\n            all_contexts = []\n\n\n            if not self.force_random:\n                indices = torch.arange(self.flow_training_well_count, device=self.device)\n                all_bitstrings = ((indices.unsqueeze(1) &gt;&gt; torch.arange(self.spatial_dimensions - 1, -1, -1, device=self.device)) &amp; 1).float()\n\n            # first we generate training for the flow model from MCMC NUTS\n            for i in range(self.flow_training_well_count):\n                if not self.force_random:\n                    target_bits = all_bitstrings[i]\n                else:\n                    target_bits = torch.randint(0, 2, (self.spatial_dimensions,), device=self.device).float()\n\n                # modify our underlying bounds to point to the new well\n                self.set_bounds_from_bits(target_bits, loss)\n\n                # call the parent method that computes the initial positions using mcmc nuts\n                samples = self._run_multichain_mcmc(potential, protocol, loss)\n\n                all_samples.append(samples)\n                all_contexts.append(target_bits.unsqueeze(0).repeat(samples.shape[0], 1))\n\n                if i % 10 == 0:\n                    print(f\"  - Sampled well {i+1}/{self.flow_training_well_count}\")\n\n            full_data = torch.cat(all_samples, dim=0)\n            full_context = torch.cat(all_contexts, dim=0)\n\n            # update our internal mean and std\n            self.data_mean = full_data.mean(0)\n            self.data_std = full_data.std(0) + 1e-6\n\n            normalized_data = (full_data - self.data_mean) / self.data_std\n            dataset = TensorDataset(normalized_data, full_context)\n            loader = DataLoader(dataset, batch_size=self.flow_batch_size, shuffle=True)\n\n            # train the flow model\n            optimizer = torch.optim.Adam(self.flow_modules.parameters(), lr=1e-3)\n            self.flow_modules.train()\n\n            print(\"Training Conditional Flow...\")\n            best_loss = float('inf')\n            patience = 20\n            patience_counter = 0\n\n            with tqdm(range(self.flow_epochs), desc=\"Flow Training\") as pbar:\n                for epoch in pbar:\n                    epoch_loss = 0\n                    for batch_x, batch_context in loader:\n                        optimizer.zero_grad()\n                        log_prob = self.flow_dist.condition(batch_context).log_prob(batch_x) #condition instructs it to use the bitstring to choose the underlying spline flow\n                        loss_val = -log_prob.mean()\n                        loss_val.backward()\n                        optimizer.step()\n                        epoch_loss += loss_val.item()\n\n                    avg_epoch_loss = epoch_loss / len(loader)\n                    pbar.set_postfix({'loss': f'{avg_epoch_loss:.4f}'})\n\n                    # Early stopping check\n                    if avg_epoch_loss &lt; best_loss:\n                        best_loss = avg_epoch_loss\n                        patience_counter = 0\n                    else:\n                        patience_counter += 1\n                        if patience_counter &gt;= patience:\n                            print(f\"Early stopping at epoch {epoch} with loss {best_loss:.4f}\")\n                            break\n\n            self.is_trained = True\n\n        finally:\n            # restore the original number of samples and bounds\n            self.samples_per_well = original_samples_per_well\n            self.starting_bounds = original_bounds_backup\n            self.num_samples = original_num_samples\n\n    def generate_initial_conditions(self, potential: Potential, protocol: Protocol, loss: Loss) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Generates initial conditions using the trained flow model.\n\n        Args:\n            potential: The potential energy object.\n            protocol: The protocol object.\n            loss: The loss object.\n\n        Returns:\n            Tuple of (initial_pos, initial_vel, noise).\n        \"\"\"\n        if not self.is_trained:\n             self._train_flow(potential, protocol, loss)\n\n        if self.samples_per_well is not None and not self.force_random: # per well mode, makes sure to sample from each well a known number of times\n\n            total_samples_per_well = self.samples_per_well * self.chains_per_well\n\n            num_wells = 2 ** self.spatial_dimensions\n            indices = torch.arange(num_wells, device=self.device)\n            all_bitstrings = ((indices.unsqueeze(1) &gt;&gt; torch.arange(self.spatial_dimensions - 1, -1, -1, device=self.device)) &amp; 1).float()\n\n            context = all_bitstrings.repeat_interleave(total_samples_per_well, dim=0)\n\n            batch_size = context.shape[0]\n\n        else:\n            if self.samples_per_well is not None: #per random force\n                batch_size = 16 * self.samples_per_well\n            else:\n                batch_size = self.num_samples\n            context = torch.randint(0, 2, (batch_size, self.spatial_dimensions), device=self.device).float()\n\n\n        # sample from the flow model\n        with torch.no_grad():\n            conditioned_flow = self.flow_dist.condition(context)\n            x_standardized = conditioned_flow.sample(torch.Size([batch_size]))\n            initial_pos = x_standardized * self.data_std + self.data_mean # unstandardize\n            # log_p_target = self._log_prob(initial_pos.unsqueeze(-1), potential, protocol) # target log probability\n\n            # log_jacobian = torch.log(self.data_std).sum()\n            # log_q_flow = conditioned_flow.log_prob(x_standardized) - log_jacobian\n            # log_weights = log_p_target - log_q_flow\n            # weights = torch.exp(log_weights - log_weights.max())\n            # weights = weights / weights.sum()\n\n        #manipulate the parent class to make sure we get the right number of samples for velocity and noise\n        original_num_samples = self.num_samples\n        self.num_samples = batch_size\n\n        initial_vel = self._get_initial_velocities()\n        noise = self._get_noise()\n\n        # restore the original number of samples for the parent class\n        self.num_samples = original_num_samples\n\n\n        return initial_pos, initial_vel, noise\n</code></pre>"},{"location":"reference/sampling/#protocolopt.sampling.ConditionalFlow.__init__","title":"<code>__init__(dt, gamma, mass, device, spatial_dimensions=1, time_steps=1000, beta=1.0, starting_bounds=None, samples_per_well=None, num_samples=5000, chains_per_well=1, warmup_ratio=0.1, min_neff=None, run_every_epoch=False, flow_layers=4, flow_epochs=300, flow_batch_size=256, flow_training_samples_per_well=500)</code>","text":"<p>Initializes the ConditionalFlow generator.</p> <p>Parameters:</p> Name Type Description Default <code>dt</code> <code>float</code> <p>Time step.</p> required <code>gamma</code> <code>float</code> <p>Friction coefficient.</p> required <code>mass</code> <code>float</code> <p>Particle mass.</p> required <code>device</code> <code>device</code> <p>Torch device.</p> required <code>spatial_dimensions</code> <code>int</code> <p>Number of spatial dimensions.</p> <code>1</code> <code>time_steps</code> <code>int</code> <p>Number of time steps.</p> <code>1000</code> <code>beta</code> <code>float</code> <p>1/kT</p> <code>1.0</code> <code>starting_bounds</code> <code>Optional[Tensor]</code> <p>Bounds for starting positions. Defaults to global bounds if None.</p> <code>None</code> <code>samples_per_well</code> <code>int</code> <p>Samples per well.</p> <code>None</code> <code>num_samples</code> <code>int</code> <p>Total samples if not per well.</p> <code>5000</code> <code>chains_per_well</code> <code>int</code> <p>Chains per well.</p> <code>1</code> <code>warmup_ratio</code> <code>float</code> <p>Ratio of warmup steps.</p> <code>0.1</code> <code>min_neff</code> <code>float</code> <p>Minimum effective sample size.</p> <code>None</code> <code>run_every_epoch</code> <code>bool</code> <p>Whether to run sampling every epoch.</p> <code>False</code> <code>flow_layers</code> <code>int</code> <p>Number of flow layers.</p> <code>4</code> <code>flow_epochs</code> <code>int</code> <p>Number of training epochs for the flow.</p> <code>300</code> <code>flow_batch_size</code> <code>int</code> <p>Batch size for flow training.</p> <code>256</code> <code>flow_training_samples_per_well</code> <code>int</code> <p>Samples per well for training the flow.</p> <code>500</code> Source code in <code>src/protocolopt/sampling/flow.py</code> <pre><code>def __init__(\n    self,\n    dt: float,\n    gamma: float,\n    mass: float,\n    device: torch.device,\n    spatial_dimensions: int = 1,\n    time_steps: int = 1000,\n    beta: float = 1.0,\n    starting_bounds: Optional[torch.Tensor] = None,\n    samples_per_well: Optional[int] = None,\n    num_samples: int = 5000,\n    chains_per_well: int = 1,\n    warmup_ratio: float = 0.1,\n    min_neff: Optional[float] = None,\n    run_every_epoch: bool = False,\n    flow_layers: int = 4,\n    flow_epochs: int = 300,\n    flow_batch_size: int = 256,\n    flow_training_samples_per_well: int = 500\n) -&gt; None:\n    \"\"\"Initializes the ConditionalFlow generator.\n\n    Args:\n        dt (float): Time step.\n        gamma (float): Friction coefficient.\n        mass (float): Particle mass.\n        device (torch.device): Torch device.\n        spatial_dimensions (int): Number of spatial dimensions.\n        time_steps (int): Number of time steps.\n        beta (float): 1/kT\n        starting_bounds (Optional[torch.Tensor]): Bounds for starting positions. Defaults to global bounds if None.\n        samples_per_well (int): Samples per well.\n        num_samples (int): Total samples if not per well.\n        chains_per_well (int): Chains per well.\n        warmup_ratio (float): Ratio of warmup steps.\n        min_neff (float): Minimum effective sample size.\n        run_every_epoch (bool): Whether to run sampling every epoch.\n        flow_layers (int): Number of flow layers.\n        flow_epochs (int): Number of training epochs for the flow.\n        flow_batch_size (int): Batch size for flow training.\n        flow_training_samples_per_well (int): Samples per well for training the flow.\n    \"\"\"\n    nn.Module.__init__(self)\n    super().__init__(\n        dt=dt,\n        gamma=gamma,\n        mass=mass,\n        device=device,\n        spatial_dimensions=spatial_dimensions,\n        time_steps=time_steps,\n        beta=beta,\n        starting_bounds=starting_bounds,\n        samples_per_well=samples_per_well,\n        num_samples=num_samples,\n        chains_per_well=chains_per_well,\n        warmup_ratio=warmup_ratio,\n        min_neff=min_neff,\n        run_every_epoch=run_every_epoch\n    )\n\n    if self.spatial_dimensions &gt; 8:\n        print('WARNING: when spatial dimensions are greater than 8 the number of samples is forced to 8 * samples_per_well and is taken randomly from the global bounds. You are NOT guaranteed to have samples from each well each epoch nor the normalizing flow to properly learn the entire bitstring space.')\n        self.force_random = True\n    else:\n        self.force_random = False\n\n    self.context_dim = self.spatial_dimensions\n\n    self.original_bounds = self.starting_bounds.clone()\n\n    transforms = []\n    if flow_layers &lt; 2:\n        raise ValueError(\"Flow layers must be at least 2 and at least 4 is highly recommended\")\n    for _ in range(flow_layers):\n        c1 = T.conditional_spline(\n            self.spatial_dimensions,\n            context_dim=self.context_dim, # number of bits in the bitstring\n            count_bins=16, #number of bins in the spline\n            bound=3.0 #since we are flowing from a N(0, 1) this is ~99.7% of the distribution\n        ).to(device)\n\n        transforms.append(c1)\n        transforms.append(T.Permute(torch.randperm(self.spatial_dimensions, device=device)))\n\n    self.base_dist = dist.Normal(torch.zeros(self.spatial_dimensions, device=device),\n                                 torch.ones(self.spatial_dimensions, device=device))\n\n    self.flow_dist = dist.ConditionalTransformedDistribution(self.base_dist, transforms)\n    self.flow_modules = nn.ModuleList([t for t in transforms if isinstance(t, nn.Module)])\n\n    self.is_trained = False\n\n    # saving the mean and std for standardization in the model for saving and loading\n    self.register_buffer('data_mean', torch.zeros(self.spatial_dimensions))\n    self.register_buffer('data_std', torch.ones(self.spatial_dimensions))\n\n    self.flow_epochs = flow_epochs\n    self.flow_batch_size = flow_batch_size\n    self.flow_training_well_count = 2**self.spatial_dimensions\n    self.flow_training_samples_per_well = flow_training_samples_per_well\n\n    self.hparams.update({\n        'force_random': self.force_random,\n        'context_dim': self.context_dim,\n        'flow_epochs': self.flow_epochs,\n        'flow_batch_size': self.flow_batch_size,\n        'flow_training_well_count': self.flow_training_well_count,\n        'flow_training_samples_per_well': self.flow_training_samples_per_well\n    })\n</code></pre>"},{"location":"reference/sampling/#protocolopt.sampling.ConditionalFlow.generate_initial_conditions","title":"<code>generate_initial_conditions(potential, protocol, loss)</code>","text":"<p>Generates initial conditions using the trained flow model.</p> <p>Parameters:</p> Name Type Description Default <code>potential</code> <code>Potential</code> <p>The potential energy object.</p> required <code>protocol</code> <code>Protocol</code> <p>The protocol object.</p> required <code>loss</code> <code>Loss</code> <p>The loss object.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor]</code> <p>Tuple of (initial_pos, initial_vel, noise).</p> Source code in <code>src/protocolopt/sampling/flow.py</code> <pre><code>def generate_initial_conditions(self, potential: Potential, protocol: Protocol, loss: Loss) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Generates initial conditions using the trained flow model.\n\n    Args:\n        potential: The potential energy object.\n        protocol: The protocol object.\n        loss: The loss object.\n\n    Returns:\n        Tuple of (initial_pos, initial_vel, noise).\n    \"\"\"\n    if not self.is_trained:\n         self._train_flow(potential, protocol, loss)\n\n    if self.samples_per_well is not None and not self.force_random: # per well mode, makes sure to sample from each well a known number of times\n\n        total_samples_per_well = self.samples_per_well * self.chains_per_well\n\n        num_wells = 2 ** self.spatial_dimensions\n        indices = torch.arange(num_wells, device=self.device)\n        all_bitstrings = ((indices.unsqueeze(1) &gt;&gt; torch.arange(self.spatial_dimensions - 1, -1, -1, device=self.device)) &amp; 1).float()\n\n        context = all_bitstrings.repeat_interleave(total_samples_per_well, dim=0)\n\n        batch_size = context.shape[0]\n\n    else:\n        if self.samples_per_well is not None: #per random force\n            batch_size = 16 * self.samples_per_well\n        else:\n            batch_size = self.num_samples\n        context = torch.randint(0, 2, (batch_size, self.spatial_dimensions), device=self.device).float()\n\n\n    # sample from the flow model\n    with torch.no_grad():\n        conditioned_flow = self.flow_dist.condition(context)\n        x_standardized = conditioned_flow.sample(torch.Size([batch_size]))\n        initial_pos = x_standardized * self.data_std + self.data_mean # unstandardize\n        # log_p_target = self._log_prob(initial_pos.unsqueeze(-1), potential, protocol) # target log probability\n\n        # log_jacobian = torch.log(self.data_std).sum()\n        # log_q_flow = conditioned_flow.log_prob(x_standardized) - log_jacobian\n        # log_weights = log_p_target - log_q_flow\n        # weights = torch.exp(log_weights - log_weights.max())\n        # weights = weights / weights.sum()\n\n    #manipulate the parent class to make sure we get the right number of samples for velocity and noise\n    original_num_samples = self.num_samples\n    self.num_samples = batch_size\n\n    initial_vel = self._get_initial_velocities()\n    noise = self._get_noise()\n\n    # restore the original number of samples for the parent class\n    self.num_samples = original_num_samples\n\n\n    return initial_pos, initial_vel, noise\n</code></pre>"},{"location":"reference/sampling/#protocolopt.sampling.ConditionalFlow.set_bounds_from_bits","title":"<code>set_bounds_from_bits(target_bitstring, loss)</code>","text":"<p>Updates sampling bounds to target a specific bitstring well.</p> <p>Parameters:</p> Name Type Description Default <code>target_bitstring</code> <code>Tensor</code> <p>Tensor representing the target bitstring (e.g., [0, 1]).</p> required <code>loss</code> <code>Loss</code> <p>Loss object containing midpoint information.</p> required Source code in <code>src/protocolopt/sampling/flow.py</code> <pre><code>def set_bounds_from_bits(self, target_bitstring: torch.Tensor, loss: Loss) -&gt; None:\n    \"\"\"Updates sampling bounds to target a specific bitstring well.\n\n    Args:\n        target_bitstring: Tensor representing the target bitstring (e.g., [0, 1]).\n        loss: Loss object containing midpoint information.\n    \"\"\"\n    if not hasattr(loss, 'midpoints'):\n        raise RuntimeError(\"Loss object must have .midpoints to define wells.\")\n\n    midpoints = loss.midpoints\n    global_bounds = self.original_bounds\n\n    new_bounds = []\n\n    for dim_idx in range(self.spatial_dimensions):\n        bit = target_bitstring[dim_idx]\n        low, high = global_bounds[dim_idx]\n        mid = midpoints[dim_idx]\n\n        if bit == 0:\n            new_bounds.append([low, mid])\n        elif bit == 1:\n            new_bounds.append([mid, high])\n        else:\n            raise ValueError(f\"Bit must be 0 or 1, got {bit}\")\n\n    # update the bounds used by the parent class\n    self.starting_bounds = torch.tensor(new_bounds, device=self.device)\n\n    # runs the parent class in global mode on this subset, basically redoing the per well logic but packed in a way the flow model needs to learn from\n    self.samples_per_well = None\n</code></pre>"},{"location":"reference/sampling/#protocolopt.sampling.LaplaceApproximation","title":"<code>LaplaceApproximation</code>","text":"<p>               Bases: <code>InitialConditionGenerator</code></p> <p>Initial condition generator using Laplace Approximation around potential minima.</p> Source code in <code>src/protocolopt/sampling/laplace.py</code> <pre><code>class LaplaceApproximation(InitialConditionGenerator):\n    \"\"\"Initial condition generator using Laplace Approximation around potential minima.\"\"\"\n\n    def __init__(\n        self, \n        dt: float, \n        gamma: float, \n        mass: float, \n        centers: torch.Tensor, \n        device: torch.device,\n        beta: float = 1.0,\n        spatial_dimensions: int = 1, \n        time_steps: int = 1000,\n        num_samples: int = 1000\n    ) -&gt; None:\n        \"\"\"Initializes LaplaceApproximation.\n\n        Args:\n            dt (float): Time step.\n            gamma (float): Friction coefficient.\n            mass (float): Particle mass.\n            centers (torch.Tensor): Tensor of center locations for approximation.\n            device (torch.device): Torch device.\n            beta (float): 1/kT\n            spatial_dimensions (int): Number of spatial dimensions.\n            time_steps (int): Number of time steps.\n            num_samples (int): Number of samples to generate.\n        \"\"\"\n        self.centers = centers\n        self.device = device\n        self.dt = dt\n        self.gamma = gamma\n        self.mass = mass\n        self.beta = beta\n        self.spatial_dimensions = spatial_dimensions\n        self.time_steps = time_steps\n        self.num_samples = num_samples\n\n        self.noise_sigma = torch.sqrt(torch.tensor(2 * self.gamma / self.beta))\n\n        self.hparams = {\n            'centers_shape': list(self.centers.shape),\n            'dt': self.dt,\n            'gamma': self.gamma,\n            'mass': self.mass,\n            'beta': self.beta,\n            'spatial_dimensions': self.spatial_dimensions,\n            'time_steps': self.time_steps,\n            'num_samples': self.num_samples,\n            'name': self.__class__.__name__\n        }\n\n    def _solve_landscape(self, potential: Potential, protocol: Protocol) -&gt; None:\n        \"\"\"Computes the Hessian and log weights at the centers.\"\"\"\n        coeff_at_t0 = protocol.get_protocol_tensor()[:, 0]\n        def potential_kernel(x):\n            return potential.potential_value(x, coeff_at_t0)\n\n        batched_hessian_func = vmap(hessian(potential_kernel), in_dims = 0)\n\n        hessian_at_centers = batched_hessian_func(self.centers) # Nwells x spatial_dimensions x spatial_dimensions\n        sign, logabsdet = torch.linalg.slogdet(hessian_at_centers)\n        if not (sign == 1).all():\n            raise ValueError(f\"Some hessians ({len(hessian_at_centers[sign == -1])}/{len(hessian_at_centers):.2f}%) are not positive definite, some of your bit centers are unstable\")\n        if not (logabsdet &gt; math.log(1e-5)).all():\n            raise ValueError(f\"Some hessians ({len(logabsdet[logabsdet &lt;= 0])}/{len(logabsdet):.2f}%) have too near zero determinant (1e-5 cutoff), some of your bit centers are unsuitable for this method. Request better handling of this if needed.\")\n        # Log P = -E - 0.5 * log|H|\n        self.log_weights = -potential_kernel(self.centers) - 0.5 * logabsdet\n        self.hessian_at_centers = hessian_at_centers\n\n    def _get_initial_velocities(self) -&gt; torch.Tensor:\n        \"\"\"Generates initial velocities.\"\"\"\n        # P(v) = exp(-beta * E_k)\n        # E_k = 1/2 * m * v^2\n        # P(v) = exp(-beta * 1/2 * m * v^2)\n        # generally P(v) = exp(-v^2 * 1/2 / sigma^2)\n        # v^2 * 1/2 / sigma^2 = beta * 1/2 * m * v^2\n        # sigma^2 = 1 / (beta * m)\n        # sigma = sqrt(1 / (beta * m))\n\n        # we draw from randn which produces N(0, 1) and scale by sigma to get N(0, sigma)\n        var = 1 / (self.beta * self.mass)\n        samples = torch.randn(self.num_samples, self.spatial_dimensions, device=self.device) * math.sqrt(var)\n        return samples\n\n    def _get_noise(self) -&gt; torch.Tensor:\n        \"\"\"Generates noise.\"\"\"\n        samples = torch.randn(self.num_samples, self.spatial_dimensions, self.time_steps, device=self.device) * self.noise_sigma * math.sqrt(self.dt)\n        return samples\n\n    def _get_initial_positions(self) -&gt; torch.Tensor:\n        \"\"\"Generates initial positions based on Laplace approximation.\"\"\"\n        with torch.no_grad():\n            chosen_centers = dist.Categorical(self.log_weights).sample((self.num_samples,))\n            center_locs = self.centers[chosen_centers]\n            chosen_H = self.hessian_at_centers[chosen_centers]\n\n            # Laplaces approximation, x ~ N(x_0, H^-1)\n            # precision matrix will invert the hessian internally\n            samples = dist.MultivariateNormal(center_locs, precision_matrix = chosen_H).sample()\n            return samples\n\n    def generate_initial_conditions(self, potential: Potential, protocol: Protocol, loss: Loss) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Generates initial conditions.\n\n        Args:\n            potential: The potential energy object.\n            protocol: The protocol object.\n            loss: The loss object.\n\n        Returns:\n            Tuple of (initial_pos, initial_vel, noise).\n        \"\"\"\n        if not hasattr(self, 'log_weights'):\n            print(\"Solving landscape for Laplace approximation...\")\n            self._solve_landscape(potential, protocol)\n            print(\"Landscape solved\")\n\n        initial_pos = self._get_initial_positions()\n        initial_vel = self._get_initial_velocities()\n        noise = self._get_noise()\n        return initial_pos, initial_vel, noise\n</code></pre>"},{"location":"reference/sampling/#protocolopt.sampling.LaplaceApproximation.__init__","title":"<code>__init__(dt, gamma, mass, centers, device, beta=1.0, spatial_dimensions=1, time_steps=1000, num_samples=1000)</code>","text":"<p>Initializes LaplaceApproximation.</p> <p>Parameters:</p> Name Type Description Default <code>dt</code> <code>float</code> <p>Time step.</p> required <code>gamma</code> <code>float</code> <p>Friction coefficient.</p> required <code>mass</code> <code>float</code> <p>Particle mass.</p> required <code>centers</code> <code>Tensor</code> <p>Tensor of center locations for approximation.</p> required <code>device</code> <code>device</code> <p>Torch device.</p> required <code>beta</code> <code>float</code> <p>1/kT</p> <code>1.0</code> <code>spatial_dimensions</code> <code>int</code> <p>Number of spatial dimensions.</p> <code>1</code> <code>time_steps</code> <code>int</code> <p>Number of time steps.</p> <code>1000</code> <code>num_samples</code> <code>int</code> <p>Number of samples to generate.</p> <code>1000</code> Source code in <code>src/protocolopt/sampling/laplace.py</code> <pre><code>def __init__(\n    self, \n    dt: float, \n    gamma: float, \n    mass: float, \n    centers: torch.Tensor, \n    device: torch.device,\n    beta: float = 1.0,\n    spatial_dimensions: int = 1, \n    time_steps: int = 1000,\n    num_samples: int = 1000\n) -&gt; None:\n    \"\"\"Initializes LaplaceApproximation.\n\n    Args:\n        dt (float): Time step.\n        gamma (float): Friction coefficient.\n        mass (float): Particle mass.\n        centers (torch.Tensor): Tensor of center locations for approximation.\n        device (torch.device): Torch device.\n        beta (float): 1/kT\n        spatial_dimensions (int): Number of spatial dimensions.\n        time_steps (int): Number of time steps.\n        num_samples (int): Number of samples to generate.\n    \"\"\"\n    self.centers = centers\n    self.device = device\n    self.dt = dt\n    self.gamma = gamma\n    self.mass = mass\n    self.beta = beta\n    self.spatial_dimensions = spatial_dimensions\n    self.time_steps = time_steps\n    self.num_samples = num_samples\n\n    self.noise_sigma = torch.sqrt(torch.tensor(2 * self.gamma / self.beta))\n\n    self.hparams = {\n        'centers_shape': list(self.centers.shape),\n        'dt': self.dt,\n        'gamma': self.gamma,\n        'mass': self.mass,\n        'beta': self.beta,\n        'spatial_dimensions': self.spatial_dimensions,\n        'time_steps': self.time_steps,\n        'num_samples': self.num_samples,\n        'name': self.__class__.__name__\n    }\n</code></pre>"},{"location":"reference/sampling/#protocolopt.sampling.LaplaceApproximation.generate_initial_conditions","title":"<code>generate_initial_conditions(potential, protocol, loss)</code>","text":"<p>Generates initial conditions.</p> <p>Parameters:</p> Name Type Description Default <code>potential</code> <code>Potential</code> <p>The potential energy object.</p> required <code>protocol</code> <code>Protocol</code> <p>The protocol object.</p> required <code>loss</code> <code>Loss</code> <p>The loss object.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor]</code> <p>Tuple of (initial_pos, initial_vel, noise).</p> Source code in <code>src/protocolopt/sampling/laplace.py</code> <pre><code>def generate_initial_conditions(self, potential: Potential, protocol: Protocol, loss: Loss) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Generates initial conditions.\n\n    Args:\n        potential: The potential energy object.\n        protocol: The protocol object.\n        loss: The loss object.\n\n    Returns:\n        Tuple of (initial_pos, initial_vel, noise).\n    \"\"\"\n    if not hasattr(self, 'log_weights'):\n        print(\"Solving landscape for Laplace approximation...\")\n        self._solve_landscape(potential, protocol)\n        print(\"Landscape solved\")\n\n    initial_pos = self._get_initial_positions()\n    initial_vel = self._get_initial_velocities()\n    noise = self._get_noise()\n    return initial_pos, initial_vel, noise\n</code></pre>"},{"location":"reference/sampling/#protocolopt.sampling.McmcNuts","title":"<code>McmcNuts</code>","text":"<p>               Bases: <code>InitialConditionGenerator</code></p> <p>Initial condition generator using MCMC with the NUTS sampler.</p> Source code in <code>src/protocolopt/sampling/mcmc.py</code> <pre><code>class McmcNuts(InitialConditionGenerator):\n    \"\"\"Initial condition generator using MCMC with the NUTS sampler.\"\"\"\n\n    def __init__(\n        self,\n        dt: float,\n        gamma: float,\n        mass: float,\n        device: torch.device,\n        spatial_dimensions: int = 1,\n        time_steps: int = 1000,\n        beta: float = 1.0,\n        starting_bounds: Optional[torch.Tensor] = None,\n        samples_per_well: Optional[int] = None,\n        num_samples: int = 5000,\n        chains_per_well: int = 1,\n        warmup_ratio: float = 0.1,\n        min_neff: Optional[float] = None,\n        run_every_epoch: bool = False\n    ) -&gt; None:\n        \"\"\"Initializes McmcNuts.\n\n        Args:\n            dt (float): Time step.\n            gamma (float): Friction coefficient.\n            mass (float): Particle mass.\n            device (torch.device): Calculation device.\n            spatial_dimensions (int): Number of spatial dimensions.\n            time_steps (int): Number of time steps.\n            beta (float): 1/kT\n            starting_bounds (torch.Tensor): Bounds for starting positions.\n            samples_per_well (int): Samples per well.\n            num_samples (int): Total samples if not per well.\n            chains_per_well (int): Chains per well.\n            warmup_ratio (float): Ratio of warmup steps.\n            min_neff (float): Minimum effective sample size.\n            run_every_epoch (bool): Whether to run sampling every epoch.\n        \"\"\"\n        self.device = device\n        self.spatial_dimensions = spatial_dimensions\n        self.time_steps = time_steps\n        self.warmup_ratio = warmup_ratio\n\n        if starting_bounds is None:\n             self.starting_bounds = torch.tensor([[-5, 5]] * self.spatial_dimensions, device=self.device)\n        else:\n             self.starting_bounds = starting_bounds\n\n        self.chains_per_well = chains_per_well\n        self.min_neff = min_neff\n        self.samples_per_well = samples_per_well\n\n        if self.samples_per_well is None:\n            self.num_samples = num_samples\n        else:\n            self.num_samples = self.samples_per_well * self.chains_per_well * 2**self.spatial_dimensions\n\n        self.beta = beta\n        self.gamma = gamma\n        self.dt = dt\n        self.noise_sigma = torch.sqrt(torch.tensor(2 * self.gamma / self.beta))\n        self.mass = mass\n        self.starting_pos = None\n        self.run_every_epoch = run_every_epoch\n\n        if self.run_every_epoch:\n            print(\"Warning: Running MCMC every epoch will be very slow and is not recommended for training. Please use the ConditionalFlowBoltzmannGenerator instead if your potential doesn't change at t0.\")\n\n        self.hparams = {\n            'spatial_dimensions': self.spatial_dimensions,\n            'time_steps': self.time_steps,\n            'warmup_ratio': self.warmup_ratio,\n            'starting_bounds': self.starting_bounds.tolist() if isinstance(self.starting_bounds, torch.Tensor) else self.starting_bounds,\n            'chains_per_well': self.chains_per_well,\n            'min_neff': self.min_neff,\n            'samples_per_well': self.samples_per_well,\n            'num_samples': self.num_samples,\n            'beta': self.beta,\n            'gamma': self.gamma,\n            'dt': self.dt,\n            'mass': self.mass,\n            'run_every_epoch': self.run_every_epoch,\n            'name': self.__class__.__name__\n        }\n\n    def _get_initial_velocities(self) -&gt; torch.Tensor:\n        \"\"\"Generates initial velocities from the Maxwell-Boltzmann distribution.\n\n        Returns:\n            Initial velocities tensor. Shape: (Num_Samples, Spatial_Dim).\n        \"\"\"\n        # P(v) = exp(-beta * E_k)\n        # E_k = 1/2 * m * v^2\n        # P(v) = exp(-beta * 1/2 * m * v^2)\n        # generally P(v) = exp(-v^2 * 1/2 / sigma^2)\n        # v^2 * 1/2 / sigma^2 = beta * 1/2 * m * v^2\n        # sigma^2 = 1 / (beta * m)\n        # sigma = sqrt(1 / (beta * m))\n\n        # we draw from randn which produces N(0, 1) and scale by sigma to get N(0, sigma)\n        var = 1 / (self.beta * self.mass)\n        samples = torch.randn(self.num_samples, self.spatial_dimensions, device=self.device) * math.sqrt(var)\n        return samples\n\n    def _get_noise(self) -&gt; torch.Tensor:\n        \"\"\"Generates noise for the simulation.\n\n        Returns:\n            Noise tensor. Shape: (Num_Samples, Spatial_Dim, Time_Steps).\n        \"\"\"\n        samples = torch.randn(self.num_samples, self.spatial_dimensions, self.time_steps, device=self.device) * self.noise_sigma * math.sqrt(self.dt)\n        return samples\n\n    def _run_multichain_mcmc(self, potential: Potential, protocol: Protocol, loss: Loss) -&gt; torch.Tensor:\n        \"\"\"Run MCMC sampling, either per-well or global depending on parameters.\n\n        Args:\n            potential: The potential energy object.\n            protocol: The protocol object providing coefficients at t=0.\n            loss: The loss object (used for midpoints if per-well sampling).\n\n        Returns:\n            Sampled positions. Shape: (Num_Samples, Spatial_Dim).\n        \"\"\"\n\n        max_attempts = 5\n\n        if self.samples_per_well is not None:\n            # Per-well sampling mode\n            well_bounds = self._partition_bounds_by_midpoints(loss)\n            all_samples = []\n\n            for well_idx, bounds in enumerate(well_bounds):\n\n                samples_per_chain = self.samples_per_well // self.chains_per_well\n\n                for attempt in range(max_attempts):\n                    well_samples = []\n\n                    # Run chains\n                    for chain_id in range(self.chains_per_well):\n                        sampler = MCMC(\n                            NUTS(lambda: self._posterior_for_mcmc(bounds[:, 0], bounds[:, 1], potential, protocol)),\n                            num_samples=samples_per_chain,\n                            warmup_steps=int(self.warmup_ratio * samples_per_chain)\n                        )\n                        sampler.run()\n                        well_samples.append(sampler.get_samples()['x'])\n\n                    # Check Neff if required\n                    current_samples = torch.cat(well_samples, dim=0)\n\n                    if self.min_neff is None:\n                        break\n\n                    # we stack the chains together to vectorize the Neff check\n                    chain_stack = torch.stack(well_samples, dim=0)\n                    neff_per_dim = pyro.ops.stats.effective_sample_size(chain_stack, chain_dim=0, sample_dim=1)\n                    min_observed_neff = neff_per_dim.min().item()\n\n                    print(f\"Well {well_idx} Attempt {attempt+1}: Neff = {min_observed_neff:.2f} (Target: {self.min_neff})\")\n\n                    if min_observed_neff &gt;= self.min_neff:\n                        break\n\n                    # Need more samples\n                    if attempt &lt; max_attempts - 1:\n                        ratio = self.min_neff / max(min_observed_neff, 1e-6)\n                        # scale factor, bounded to avoid explosion but ensure progress\n                        scale_factor = max(1.1, min(ratio, 5.0))\n                        new_samples_per_chain = int(samples_per_chain * scale_factor)\n                        print(f\"  - Neff insufficient. Increasing samples per chain from {samples_per_chain} to {new_samples_per_chain}\")\n                        samples_per_chain = new_samples_per_chain\n                    else:\n                        print(f\"  - WARNING: Max attempts reached for Well {well_idx}. Neff {min_observed_neff:.2f} &lt; {self.min_neff}\")\n\n                # Print Neff statistics for the final samples of this well\n                # Re-calculate since we might have broken out of loop\n                chain_stack = torch.stack(well_samples, dim=0)\n                neff_per_dim = pyro.ops.stats.effective_sample_size(chain_stack, chain_dim=0, sample_dim=1)\n                min_neff = neff_per_dim.min().item()\n                mean_neff = neff_per_dim.mean().item()\n                print(f\"  Well {well_idx} Stats: Min Neff: {min_neff:.2f}, Mean Neff: {mean_neff:.2f}\")\n\n                all_samples.append(current_samples)\n\n            self.starting_pos = torch.cat(all_samples, dim=0)\n        else:\n            # Legacy mode: global sampling\n            all_samples = []\n            num_chains = self.chains_per_well\n            samples_per_chain = self.num_samples // num_chains\n\n            bounds_low = self.starting_bounds[:, 0]\n            bounds_high = self.starting_bounds[:, 1]\n\n            for attempt in range(max_attempts):\n                chain_samples_list = []\n\n                for chain_id in range(num_chains):\n                    sampler = MCMC(\n                        NUTS(lambda: self._posterior_for_mcmc(bounds_low, bounds_high, potential, protocol)),\n                        num_samples=samples_per_chain,\n                        warmup_steps=int(self.warmup_ratio * samples_per_chain)\n                    )\n                    sampler.run()\n                    chain_samples_list.append(sampler.get_samples()['x'])\n\n                # Check Neff\n                if self.min_neff is None:\n                    all_samples = chain_samples_list\n                    break\n\n                chain_stack = torch.stack(chain_samples_list, dim=0)\n                neff_per_dim = pyro.ops.stats.effective_sample_size(chain_stack, chain_dim=0, sample_dim=1)\n                min_observed_neff = neff_per_dim.min().item()\n\n                print(f\"Global Sampling Attempt {attempt+1}: Neff = {min_observed_neff:.2f} (Target: {self.min_neff})\")\n\n                if min_observed_neff &gt;= self.min_neff:\n                    all_samples = chain_samples_list\n                    break\n\n                if attempt &lt; max_attempts - 1:\n                    ratio = self.min_neff / max(min_observed_neff, 1e-6)\n                    scale_factor = max(1.1, min(ratio, 5.0))\n                    new_samples_per_chain = int(samples_per_chain * scale_factor)\n                    print(f\"  - Neff insufficient. Increasing samples per chain from {samples_per_chain} to {new_samples_per_chain}\")\n                    samples_per_chain = new_samples_per_chain\n                else:\n                    print(f\"  - WARNING: Max attempts reached. Neff {min_observed_neff:.2f} &lt; {self.min_neff}\")\n                    all_samples = chain_samples_list\n\n            # Print final stats for global sampling\n            if len(all_samples) &gt; 0:\n                 chain_stack = torch.stack(all_samples, dim=0)\n                 neff_per_dim = pyro.ops.stats.effective_sample_size(chain_stack, chain_dim=0, sample_dim=1)\n                 min_neff = neff_per_dim.min().item()\n                 mean_neff = neff_per_dim.mean().item()\n                 print(f\"  MCMC Batch Stats: Min Neff: {min_neff:.2f}, Mean Neff: {mean_neff:.2f}\")\n\n            self.starting_pos = torch.cat(all_samples, dim=0)\n        return self.starting_pos\n\n    def _log_prob(self, state_vectors: torch.Tensor, potential: Potential, protocol: Protocol) -&gt; torch.Tensor:\n        #exp(-beta * U), boltzman distribution assumed for posterior\n        coeff_at_t0 = protocol.get_protocol_tensor()[:, 0]\n        return -self.beta * potential.potential_value(state_vectors, coeff_at_t0)\n\n    def _posterior_for_mcmc(self, bounds_low: torch.Tensor, bounds_high: torch.Tensor, potential: Potential, protocol: Protocol) -&gt; None:\n        \"\"\"Wraps the potential energy function into a Pyro-compatible factor for NUTS sampling.\"\"\"\n        x = pyro.sample(\"x\", pyro.distributions.Uniform(bounds_low, bounds_high).to_event(1))\n        pyro.factor(\"logp\", self._log_prob(x.unsqueeze(-1), potential, protocol))\n\n    def _partition_bounds_by_midpoints(self, loss: Loss) -&gt; List[torch.Tensor]:\n        \"\"\"Partition the sampling bounds using midpoints to create per-well regions\"\"\"\n\n        if not hasattr(loss, 'midpoints'):\n            return [self.starting_bounds]\n\n        midpoints = loss.midpoints\n        bounds = self.starting_bounds\n\n        dim_segments = []\n        for dim_idx in range(self.spatial_dimensions):\n            low = bounds[dim_idx, 0]\n            high = bounds[dim_idx, 1]\n            mid = midpoints[dim_idx]\n            dim_segments.append([[low, mid], [mid, high]])\n        well_bounds = []\n        for combination in itertools.product(*dim_segments):\n            well_bound = torch.stack([torch.tensor(seg, device=self.device) for seg in combination])\n            well_bounds.append(well_bound)\n        return well_bounds\n\n    def generate_initial_conditions(self, potential: Potential, protocol: Protocol, loss: Loss) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Generates initial conditions (positions, velocities, noise).\n\n        Args:\n            potential: The potential energy object.\n            protocol: The protocol object.\n            loss: The loss object.\n\n        Returns:\n            Tuple of (initial_pos, initial_vel, noise).\n        \"\"\"\n        if self.run_every_epoch:\n            initial_pos = self.starting_pos\n        else:\n            initial_pos = self._run_multichain_mcmc(potential, protocol, loss)\n        initial_vel = self._get_initial_velocities()\n        noise = self._get_noise()\n        return initial_pos, initial_vel, noise\n</code></pre>"},{"location":"reference/sampling/#protocolopt.sampling.McmcNuts.__init__","title":"<code>__init__(dt, gamma, mass, device, spatial_dimensions=1, time_steps=1000, beta=1.0, starting_bounds=None, samples_per_well=None, num_samples=5000, chains_per_well=1, warmup_ratio=0.1, min_neff=None, run_every_epoch=False)</code>","text":"<p>Initializes McmcNuts.</p> <p>Parameters:</p> Name Type Description Default <code>dt</code> <code>float</code> <p>Time step.</p> required <code>gamma</code> <code>float</code> <p>Friction coefficient.</p> required <code>mass</code> <code>float</code> <p>Particle mass.</p> required <code>device</code> <code>device</code> <p>Calculation device.</p> required <code>spatial_dimensions</code> <code>int</code> <p>Number of spatial dimensions.</p> <code>1</code> <code>time_steps</code> <code>int</code> <p>Number of time steps.</p> <code>1000</code> <code>beta</code> <code>float</code> <p>1/kT</p> <code>1.0</code> <code>starting_bounds</code> <code>Tensor</code> <p>Bounds for starting positions.</p> <code>None</code> <code>samples_per_well</code> <code>int</code> <p>Samples per well.</p> <code>None</code> <code>num_samples</code> <code>int</code> <p>Total samples if not per well.</p> <code>5000</code> <code>chains_per_well</code> <code>int</code> <p>Chains per well.</p> <code>1</code> <code>warmup_ratio</code> <code>float</code> <p>Ratio of warmup steps.</p> <code>0.1</code> <code>min_neff</code> <code>float</code> <p>Minimum effective sample size.</p> <code>None</code> <code>run_every_epoch</code> <code>bool</code> <p>Whether to run sampling every epoch.</p> <code>False</code> Source code in <code>src/protocolopt/sampling/mcmc.py</code> <pre><code>def __init__(\n    self,\n    dt: float,\n    gamma: float,\n    mass: float,\n    device: torch.device,\n    spatial_dimensions: int = 1,\n    time_steps: int = 1000,\n    beta: float = 1.0,\n    starting_bounds: Optional[torch.Tensor] = None,\n    samples_per_well: Optional[int] = None,\n    num_samples: int = 5000,\n    chains_per_well: int = 1,\n    warmup_ratio: float = 0.1,\n    min_neff: Optional[float] = None,\n    run_every_epoch: bool = False\n) -&gt; None:\n    \"\"\"Initializes McmcNuts.\n\n    Args:\n        dt (float): Time step.\n        gamma (float): Friction coefficient.\n        mass (float): Particle mass.\n        device (torch.device): Calculation device.\n        spatial_dimensions (int): Number of spatial dimensions.\n        time_steps (int): Number of time steps.\n        beta (float): 1/kT\n        starting_bounds (torch.Tensor): Bounds for starting positions.\n        samples_per_well (int): Samples per well.\n        num_samples (int): Total samples if not per well.\n        chains_per_well (int): Chains per well.\n        warmup_ratio (float): Ratio of warmup steps.\n        min_neff (float): Minimum effective sample size.\n        run_every_epoch (bool): Whether to run sampling every epoch.\n    \"\"\"\n    self.device = device\n    self.spatial_dimensions = spatial_dimensions\n    self.time_steps = time_steps\n    self.warmup_ratio = warmup_ratio\n\n    if starting_bounds is None:\n         self.starting_bounds = torch.tensor([[-5, 5]] * self.spatial_dimensions, device=self.device)\n    else:\n         self.starting_bounds = starting_bounds\n\n    self.chains_per_well = chains_per_well\n    self.min_neff = min_neff\n    self.samples_per_well = samples_per_well\n\n    if self.samples_per_well is None:\n        self.num_samples = num_samples\n    else:\n        self.num_samples = self.samples_per_well * self.chains_per_well * 2**self.spatial_dimensions\n\n    self.beta = beta\n    self.gamma = gamma\n    self.dt = dt\n    self.noise_sigma = torch.sqrt(torch.tensor(2 * self.gamma / self.beta))\n    self.mass = mass\n    self.starting_pos = None\n    self.run_every_epoch = run_every_epoch\n\n    if self.run_every_epoch:\n        print(\"Warning: Running MCMC every epoch will be very slow and is not recommended for training. Please use the ConditionalFlowBoltzmannGenerator instead if your potential doesn't change at t0.\")\n\n    self.hparams = {\n        'spatial_dimensions': self.spatial_dimensions,\n        'time_steps': self.time_steps,\n        'warmup_ratio': self.warmup_ratio,\n        'starting_bounds': self.starting_bounds.tolist() if isinstance(self.starting_bounds, torch.Tensor) else self.starting_bounds,\n        'chains_per_well': self.chains_per_well,\n        'min_neff': self.min_neff,\n        'samples_per_well': self.samples_per_well,\n        'num_samples': self.num_samples,\n        'beta': self.beta,\n        'gamma': self.gamma,\n        'dt': self.dt,\n        'mass': self.mass,\n        'run_every_epoch': self.run_every_epoch,\n        'name': self.__class__.__name__\n    }\n</code></pre>"},{"location":"reference/sampling/#protocolopt.sampling.McmcNuts.generate_initial_conditions","title":"<code>generate_initial_conditions(potential, protocol, loss)</code>","text":"<p>Generates initial conditions (positions, velocities, noise).</p> <p>Parameters:</p> Name Type Description Default <code>potential</code> <code>Potential</code> <p>The potential energy object.</p> required <code>protocol</code> <code>Protocol</code> <p>The protocol object.</p> required <code>loss</code> <code>Loss</code> <p>The loss object.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor]</code> <p>Tuple of (initial_pos, initial_vel, noise).</p> Source code in <code>src/protocolopt/sampling/mcmc.py</code> <pre><code>def generate_initial_conditions(self, potential: Potential, protocol: Protocol, loss: Loss) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Generates initial conditions (positions, velocities, noise).\n\n    Args:\n        potential: The potential energy object.\n        protocol: The protocol object.\n        loss: The loss object.\n\n    Returns:\n        Tuple of (initial_pos, initial_vel, noise).\n    \"\"\"\n    if self.run_every_epoch:\n        initial_pos = self.starting_pos\n    else:\n        initial_pos = self._run_multichain_mcmc(potential, protocol, loss)\n    initial_vel = self._get_initial_velocities()\n    noise = self._get_noise()\n    return initial_pos, initial_vel, noise\n</code></pre>"},{"location":"reference/simulators/","title":"Simulators","text":""},{"location":"reference/simulators/#protocolopt.simulators.EulerMaruyama","title":"<code>EulerMaruyama</code>","text":"<p>               Bases: <code>Simulator</code></p> <p>Simulator implementation using the Euler-Maruyama method.</p> Source code in <code>src/protocolopt/simulators/euler_maruyama.py</code> <pre><code>class EulerMaruyama(Simulator):\n    \"\"\"Simulator implementation using the Euler-Maruyama method.\"\"\"\n\n    def __init__(self, mode: str, gamma: float, beta: float = 1.0, mass: float = 1.0, dt: float = None, compile_mode: bool = True) -&gt; None:\n        \"\"\"Initializes the EulerMaruyama simulator.\n\n        Args:\n            mode: Simulation mode, either 'underdamped' or 'overdamped'.\n            gamma: Friction coefficient.\n            beta: 1/kT\n            mass: Particle mass (default 1.0).\n            dt: Time step size. If None, calculated as 1/time_steps.\n            compile_mode: Whether to compile the step functions.\n\n        Raises:\n            ValueError: If mode is not 'underdamped' or 'overdamped'.\n        \"\"\"\n        if mode not in ['underdamped', 'overdamped']:\n            raise ValueError(f\"Invalid mode: {mode}, choose from 'underdamped' or 'overdamped'\")\n        self.mode = mode\n        self.dt = dt\n        self.mass = mass\n        self.gamma = gamma\n        self.beta = beta\n        self.compile_mode = compile_mode\n\n        self.noise_sigma = torch.sqrt(torch.tensor(2 * self.gamma / self.beta))\n\n        self._compiled_underdamped_step = robust_compile(self._underdamped_step, compile_mode=self.compile_mode)\n        self._compiled_overdamped_step = robust_compile(self._overdamped_step, compile_mode=self.compile_mode)\n\n        self.hparams = {\n            'mode': self.mode,\n            'gamma': self.gamma,\n            'beta': self.beta,\n            'mass': self.mass,\n            'noise_sigma': self.noise_sigma,\n            'dt': self.dt,\n            'compile_mode': self.compile_mode,\n            'name': self.__class__.__name__\n        }\n\n    @staticmethod\n    def _overdamped_step(current_pos, dv_dx, noise, dt, gamma):\n        return current_pos - (dv_dx * dt - noise) / gamma\n\n    @staticmethod\n    def _underdamped_step(current_pos, current_vel, dv_dx, noise, dt, gamma, mass):\n        next_pos = current_pos + current_vel * dt\n        next_vel = current_vel + ( -1 * gamma * current_vel * dt - dv_dx * dt + noise) / mass\n        return next_pos, next_vel\n\n    def make_microstate_paths(\n        self,\n        potential: Any,\n        initial_pos: torch.Tensor,\n        initial_vel: torch.Tensor,\n        time_steps: int,\n        noise: torch.Tensor,\n        protocol_tensor: torch.Tensor\n    ) -&gt; Tuple[MicrostatePaths, PotentialTensor, MalliavinWeight]:\n        \"\"\"Generates microstate paths based on the system dynamics.\n\n        Args:\n            potential: The potential energy landscape object.\n            initial_pos: Starting positions. Shape: (Num_Traj, Spatial_Dim).\n            initial_vel: Starting velocities. Shape: (Num_Traj, Spatial_Dim).\n            time_steps: Number of integration steps to perform.\n            noise: Noise tensor (sampled or given).\n                   Shape: (Batch, Spatial_Dim, Time_Steps)\n            protocol_tensor: Time-dependent control signals for the potential.\n                             Shape: (Control_Dim, Time_Steps+1)\n\n        Returns:\n            A tuple containing:\n            - **microstate_paths**: Full path of particles.\n                                    Shape: (Batch, Spatial_Dim, Time_Steps+1, 2)\n                                    Dimension 3 is (position, velocity).\n            - **potential_val**: Potential energy at each step.\n                                 Shape: (Batch, Time_Steps)\n            - **malliavin_weight**: Computed path weights.\n                                     Shape: (Batch, Control_Dim, Time_Steps)\n            - **dw**: Change in potential energy at each step.\n                      Shape: (Batch, Time_Steps)\n\n        Raises:\n            ValueError: If an invalid simulation mode is selected.\n        \"\"\"\n        #potential is a Potential object\n        #initial_phase of shape (num traj, spatial dimensions, 2 for position and velocity), 3 dimensions in total\n        #output of shape (initial_phase.shape[0] num traj, initial_phase.shape[1] spatial dimensions, time_steps+1, 2 for position and velocity), 4 dimensions in total\n        #and potential of shape (initial_phase.shape[0] num traj, initial_phase.shape[1] spatial dimensions, time_steps+1)\n        #and potential of shape (initial_phase.shape[0] num traj, time_steps+1) potential is scalar\n        if self.dt is None:\n            dt = 1 / time_steps\n        else:\n            dt = self.dt\n\n        # Use lists to build trajectories dynamically, preserving the computation graph\n        traj_pos_list = [initial_pos]\n        traj_vel_list = [initial_vel]\n        potential_list = []\n        dv_dxda_list = []\n        dw_list = []\n        for i in range(time_steps):\n            if self.mode == 'underdamped':\n                # dx = v * dt\n                # dv = (-gamma * v * dt - dV/dx * dt + noise) / mass\n                current_pos = traj_pos_list[-1]\n                current_vel = traj_vel_list[-1]\n\n                dv_dx = potential.dv_dx(current_pos, protocol_tensor, i)\n                U = potential.get_potential_value(current_pos, protocol_tensor, i)\n                dv_dxda = potential.dv_dxda(current_pos, protocol_tensor, i)\n\n                # Compute next positions and velocities\n                next_pos, next_vel = self._compiled_underdamped_step(\n                                    current_pos, current_vel, dv_dx, noise[..., i], dt, self.gamma, self.mass\n                                )\n\n                U_next = potential.get_potential_value(current_pos, protocol_tensor, i + 1)\n                dw_list.append(U_next - U)\n\n                traj_pos_list.append(next_pos)\n                traj_vel_list.append(next_vel)\n                potential_list.append(U.squeeze())\n                dv_dxda_list.append(dv_dxda)\n            elif self.mode == 'overdamped':\n                # dv = 0\n                # dx = - (dV/dx * dt + noise) / gamma\n\n                current_pos = traj_pos_list[-1]\n\n                dv_dx = potential.dv_dx(current_pos, protocol_tensor, i)\n                U = potential.get_potential_value(current_pos, protocol_tensor, i)\n                dv_dxda = potential.dv_dxda(current_pos, protocol_tensor, i)\n\n                next_pos = self._compiled_overdamped_step(current_pos, dv_dx, noise[..., i], dt, self.gamma)\n\n                U_next = potential.get_potential_value(current_pos, protocol_tensor, i + 1)\n                dw_list.append(U_next - U)\n\n                traj_pos_list.append(next_pos)\n                traj_vel_list.append(torch.zeros_like(next_pos))\n                potential_list.append(U.squeeze())\n                dv_dxda_list.append(dv_dxda)\n            else:\n                raise ValueError(f\"Please choose a valid mode, got {self.mode}, choose from 'underdamped' or 'overdamped'\")\n\n        traj_pos = torch.stack(traj_pos_list, dim=-1)  # (num_traj, spatial_dims, time_steps+1)\n        traj_vel = torch.stack(traj_vel_list, dim=-1)  # (num_traj, spatial_dims, time_steps+1)\n        potential_tensor = torch.stack(potential_list, dim=-1)  # (num_traj, time_steps)\n        dv_dxda_tensor = torch.stack(dv_dxda_list, dim=-1)  # (num_traj, coeff_count, time_steps)\n        dw_tensor = torch.stack(dw_list, dim=-1)  # (num_traj, time_steps)\n\n        microstate_paths = torch.cat([traj_pos.unsqueeze(-1), traj_vel.unsqueeze(-1)], dim=-1)\n        malliavin_weight = self._compute_malliavin_weight(dv_dxda_tensor, noise, self.noise_sigma)\n\n        return microstate_paths, potential_tensor, malliavin_weight, dw_tensor\n</code></pre>"},{"location":"reference/simulators/#protocolopt.simulators.EulerMaruyama.__init__","title":"<code>__init__(mode, gamma, beta=1.0, mass=1.0, dt=None, compile_mode=True)</code>","text":"<p>Initializes the EulerMaruyama simulator.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>Simulation mode, either 'underdamped' or 'overdamped'.</p> required <code>gamma</code> <code>float</code> <p>Friction coefficient.</p> required <code>beta</code> <code>float</code> <p>1/kT</p> <code>1.0</code> <code>mass</code> <code>float</code> <p>Particle mass (default 1.0).</p> <code>1.0</code> <code>dt</code> <code>float</code> <p>Time step size. If None, calculated as 1/time_steps.</p> <code>None</code> <code>compile_mode</code> <code>bool</code> <p>Whether to compile the step functions.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If mode is not 'underdamped' or 'overdamped'.</p> Source code in <code>src/protocolopt/simulators/euler_maruyama.py</code> <pre><code>def __init__(self, mode: str, gamma: float, beta: float = 1.0, mass: float = 1.0, dt: float = None, compile_mode: bool = True) -&gt; None:\n    \"\"\"Initializes the EulerMaruyama simulator.\n\n    Args:\n        mode: Simulation mode, either 'underdamped' or 'overdamped'.\n        gamma: Friction coefficient.\n        beta: 1/kT\n        mass: Particle mass (default 1.0).\n        dt: Time step size. If None, calculated as 1/time_steps.\n        compile_mode: Whether to compile the step functions.\n\n    Raises:\n        ValueError: If mode is not 'underdamped' or 'overdamped'.\n    \"\"\"\n    if mode not in ['underdamped', 'overdamped']:\n        raise ValueError(f\"Invalid mode: {mode}, choose from 'underdamped' or 'overdamped'\")\n    self.mode = mode\n    self.dt = dt\n    self.mass = mass\n    self.gamma = gamma\n    self.beta = beta\n    self.compile_mode = compile_mode\n\n    self.noise_sigma = torch.sqrt(torch.tensor(2 * self.gamma / self.beta))\n\n    self._compiled_underdamped_step = robust_compile(self._underdamped_step, compile_mode=self.compile_mode)\n    self._compiled_overdamped_step = robust_compile(self._overdamped_step, compile_mode=self.compile_mode)\n\n    self.hparams = {\n        'mode': self.mode,\n        'gamma': self.gamma,\n        'beta': self.beta,\n        'mass': self.mass,\n        'noise_sigma': self.noise_sigma,\n        'dt': self.dt,\n        'compile_mode': self.compile_mode,\n        'name': self.__class__.__name__\n    }\n</code></pre>"},{"location":"reference/simulators/#protocolopt.simulators.EulerMaruyama.make_microstate_paths","title":"<code>make_microstate_paths(potential, initial_pos, initial_vel, time_steps, noise, protocol_tensor)</code>","text":"<p>Generates microstate paths based on the system dynamics.</p> <p>Parameters:</p> Name Type Description Default <code>potential</code> <code>Any</code> <p>The potential energy landscape object.</p> required <code>initial_pos</code> <code>Tensor</code> <p>Starting positions. Shape: (Num_Traj, Spatial_Dim).</p> required <code>initial_vel</code> <code>Tensor</code> <p>Starting velocities. Shape: (Num_Traj, Spatial_Dim).</p> required <code>time_steps</code> <code>int</code> <p>Number of integration steps to perform.</p> required <code>noise</code> <code>Tensor</code> <p>Noise tensor (sampled or given).    Shape: (Batch, Spatial_Dim, Time_Steps)</p> required <code>protocol_tensor</code> <code>Tensor</code> <p>Time-dependent control signals for the potential.              Shape: (Control_Dim, Time_Steps+1)</p> required <p>Returns:</p> Type Description <code>MicrostatePaths</code> <p>A tuple containing:</p> <code>PotentialTensor</code> <ul> <li>microstate_paths: Full path of particles.                     Shape: (Batch, Spatial_Dim, Time_Steps+1, 2)                     Dimension 3 is (position, velocity).</li> </ul> <code>MalliavinWeight</code> <ul> <li>potential_val: Potential energy at each step.                  Shape: (Batch, Time_Steps)</li> </ul> <code>Tuple[MicrostatePaths, PotentialTensor, MalliavinWeight]</code> <ul> <li>malliavin_weight: Computed path weights.                      Shape: (Batch, Control_Dim, Time_Steps)</li> </ul> <code>Tuple[MicrostatePaths, PotentialTensor, MalliavinWeight]</code> <ul> <li>dw: Change in potential energy at each step.       Shape: (Batch, Time_Steps)</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid simulation mode is selected.</p> Source code in <code>src/protocolopt/simulators/euler_maruyama.py</code> <pre><code>def make_microstate_paths(\n    self,\n    potential: Any,\n    initial_pos: torch.Tensor,\n    initial_vel: torch.Tensor,\n    time_steps: int,\n    noise: torch.Tensor,\n    protocol_tensor: torch.Tensor\n) -&gt; Tuple[MicrostatePaths, PotentialTensor, MalliavinWeight]:\n    \"\"\"Generates microstate paths based on the system dynamics.\n\n    Args:\n        potential: The potential energy landscape object.\n        initial_pos: Starting positions. Shape: (Num_Traj, Spatial_Dim).\n        initial_vel: Starting velocities. Shape: (Num_Traj, Spatial_Dim).\n        time_steps: Number of integration steps to perform.\n        noise: Noise tensor (sampled or given).\n               Shape: (Batch, Spatial_Dim, Time_Steps)\n        protocol_tensor: Time-dependent control signals for the potential.\n                         Shape: (Control_Dim, Time_Steps+1)\n\n    Returns:\n        A tuple containing:\n        - **microstate_paths**: Full path of particles.\n                                Shape: (Batch, Spatial_Dim, Time_Steps+1, 2)\n                                Dimension 3 is (position, velocity).\n        - **potential_val**: Potential energy at each step.\n                             Shape: (Batch, Time_Steps)\n        - **malliavin_weight**: Computed path weights.\n                                 Shape: (Batch, Control_Dim, Time_Steps)\n        - **dw**: Change in potential energy at each step.\n                  Shape: (Batch, Time_Steps)\n\n    Raises:\n        ValueError: If an invalid simulation mode is selected.\n    \"\"\"\n    #potential is a Potential object\n    #initial_phase of shape (num traj, spatial dimensions, 2 for position and velocity), 3 dimensions in total\n    #output of shape (initial_phase.shape[0] num traj, initial_phase.shape[1] spatial dimensions, time_steps+1, 2 for position and velocity), 4 dimensions in total\n    #and potential of shape (initial_phase.shape[0] num traj, initial_phase.shape[1] spatial dimensions, time_steps+1)\n    #and potential of shape (initial_phase.shape[0] num traj, time_steps+1) potential is scalar\n    if self.dt is None:\n        dt = 1 / time_steps\n    else:\n        dt = self.dt\n\n    # Use lists to build trajectories dynamically, preserving the computation graph\n    traj_pos_list = [initial_pos]\n    traj_vel_list = [initial_vel]\n    potential_list = []\n    dv_dxda_list = []\n    dw_list = []\n    for i in range(time_steps):\n        if self.mode == 'underdamped':\n            # dx = v * dt\n            # dv = (-gamma * v * dt - dV/dx * dt + noise) / mass\n            current_pos = traj_pos_list[-1]\n            current_vel = traj_vel_list[-1]\n\n            dv_dx = potential.dv_dx(current_pos, protocol_tensor, i)\n            U = potential.get_potential_value(current_pos, protocol_tensor, i)\n            dv_dxda = potential.dv_dxda(current_pos, protocol_tensor, i)\n\n            # Compute next positions and velocities\n            next_pos, next_vel = self._compiled_underdamped_step(\n                                current_pos, current_vel, dv_dx, noise[..., i], dt, self.gamma, self.mass\n                            )\n\n            U_next = potential.get_potential_value(current_pos, protocol_tensor, i + 1)\n            dw_list.append(U_next - U)\n\n            traj_pos_list.append(next_pos)\n            traj_vel_list.append(next_vel)\n            potential_list.append(U.squeeze())\n            dv_dxda_list.append(dv_dxda)\n        elif self.mode == 'overdamped':\n            # dv = 0\n            # dx = - (dV/dx * dt + noise) / gamma\n\n            current_pos = traj_pos_list[-1]\n\n            dv_dx = potential.dv_dx(current_pos, protocol_tensor, i)\n            U = potential.get_potential_value(current_pos, protocol_tensor, i)\n            dv_dxda = potential.dv_dxda(current_pos, protocol_tensor, i)\n\n            next_pos = self._compiled_overdamped_step(current_pos, dv_dx, noise[..., i], dt, self.gamma)\n\n            U_next = potential.get_potential_value(current_pos, protocol_tensor, i + 1)\n            dw_list.append(U_next - U)\n\n            traj_pos_list.append(next_pos)\n            traj_vel_list.append(torch.zeros_like(next_pos))\n            potential_list.append(U.squeeze())\n            dv_dxda_list.append(dv_dxda)\n        else:\n            raise ValueError(f\"Please choose a valid mode, got {self.mode}, choose from 'underdamped' or 'overdamped'\")\n\n    traj_pos = torch.stack(traj_pos_list, dim=-1)  # (num_traj, spatial_dims, time_steps+1)\n    traj_vel = torch.stack(traj_vel_list, dim=-1)  # (num_traj, spatial_dims, time_steps+1)\n    potential_tensor = torch.stack(potential_list, dim=-1)  # (num_traj, time_steps)\n    dv_dxda_tensor = torch.stack(dv_dxda_list, dim=-1)  # (num_traj, coeff_count, time_steps)\n    dw_tensor = torch.stack(dw_list, dim=-1)  # (num_traj, time_steps)\n\n    microstate_paths = torch.cat([traj_pos.unsqueeze(-1), traj_vel.unsqueeze(-1)], dim=-1)\n    malliavin_weight = self._compute_malliavin_weight(dv_dxda_tensor, noise, self.noise_sigma)\n\n    return microstate_paths, potential_tensor, malliavin_weight, dw_tensor\n</code></pre>"}]}